

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/Q.png">
  <link rel="icon" href="/img/Q.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="archi">
  <meta name="keywords" content="">
  
    <meta name="description" content="TRPO论文笔记  摘要：一种策略改进过程，保证单调改进。通过对理论进行几次近似，开发了TRPO算法。该算法类似于自然梯度策略（natural policy gradient）方法。尽管近似值与理论有所偏差，但它往往能够实现单调改进，几乎不用调超参。  背景 大多数策略优化算法可以分为三类：1）策略迭代方法（policy iteration methods），即基于值函数的方法；2">
<meta property="og:type" content="article">
<meta property="og:title" content="TRPO论文笔记">
<meta property="og:url" content="https://qi-z.cn/2025/01/09/TRPO/index.html">
<meta property="og:site_name" content="Q&#39;s Blog">
<meta property="og:description" content="TRPO论文笔记  摘要：一种策略改进过程，保证单调改进。通过对理论进行几次近似，开发了TRPO算法。该算法类似于自然梯度策略（natural policy gradient）方法。尽管近似值与理论有所偏差，但它往往能够实现单调改进，几乎不用调超参。  背景 大多数策略优化算法可以分为三类：1）策略迭代方法（policy iteration methods），即基于值函数的方法；2">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-01-09T13:10:07.274Z">
<meta property="article:modified_time" content="2025-01-09T13:51:05.423Z">
<meta property="article:author" content="archi">
<meta property="article:tag" content="MARL">
<meta property="article:tag" content="论文笔记">
<meta name="twitter:card" content="summary_large_image">
  
  
  <title>TRPO论文笔记 - Q&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"qi-z.cn","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":100,"cursorChar":"","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":"G-HB7P267418","tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <header style="height: 90vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Q&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/archi-z/image_hosting/Blog/Backgrounds/202201182008993.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="TRPO论文笔记">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2025-01-09 21:10" pubdate>
        2025年1月9日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.1k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      60 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="iconfont icon-arrowdown"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">TRPO论文笔记</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2025年1月9日 晚上
                
              </p>
            
            <div class="markdown-body">
              <h1 id="trpo论文笔记">TRPO论文笔记</h1>
<blockquote>
<p>摘要：一种策略改进过程，<strong>保证单调改进</strong>。通过对理论进行几次近似，开发了TRPO算法。该算法类似于自然梯度策略（natural
policy
gradient）方法。尽管近似值与理论有所偏差，但它往往能够实现单调改进，几乎不用调超参。</p>
</blockquote>
<h2 id="背景">背景</h2>
<p>大多数策略优化算法可以分为三类：1）策略迭代方法（policy iteration
methods），即基于值函数的方法；2）策略梯度方法（policy gradient
methods），即参数化策略的方法；3）无导数优化方法（derivative-free
optimization methods）。</p>
<p>无导数优化方法在很多问题上受到青睐，因为它们效果好，容易理解和实现。ADP（approximate
dynamic
programming）和基于梯度的方法无法击败无导数优化方法，这是令人不满意的，因为基于梯度的优化算法比无梯度方法享有更好的样本复杂度保证。基于梯度的优化方法在监督学习的函数估计中已经比较成功，如何将其扩展到强化学习？</p>
<p>文章证明<strong>最小化某个替代目标函数可以保证策略改进</strong>。通过对理论算法进行一系列近似，文章给出了TRPO算法。文章给出了这个算法的两种变体——single
path和vine。前者可以应用于model-free场景，后者需要将系统恢复到特定状态（这通常只能在模拟中实现）。</p>
<h2 id="预备知识">预备知识</h2>
<p>基于策略<span class="math inline">\(\pi\)</span>的期望收益<span
class="math inline">\(\eta(\pi)\)</span>和优势函数<span
class="math inline">\(A_{\pi}(s_{t},a_{t})\)</span>，计算策略<span
class="math inline">\(\widetilde{\pi}\)</span>的期望收益<span
class="math inline">\(\eta(\widetilde{\pi})\)</span>，公式如下： <span
class="math display">\[
\eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,...\sim\color{red}\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\color{red}\pi}(s_{t},a_{t})]
\tag{1}
\]</span></p>
<blockquote>
<p>证明：</p>
</blockquote>
<p>定义策略<span class="math inline">\(\pi\)</span>下状态<span
class="math inline">\(s\)</span>的<strong>折扣状态访问频率（discounted
visitation frequencies）</strong>为<span
class="math inline">\(\rho_{\pi}(s)\)</span>如下： <span
class="math display">\[
\rho_{\pi}(s)=P(s_{0}=s)+\gamma P(s_{1}=s)+\gamma^{2}P(s_{2}=s)+\dots
\tag{2}
\]</span></p>
<blockquote>
<p>上面的折扣状态访问频率未归一化，归一化形式为<span
class="math inline">\(\rho_{\pi}(s) = (1 - \gamma) \sum_{t=0}^{\infty}
\gamma^{t} P(s_{t}=s)\)</span></p>
</blockquote>
<p>基于折扣状态访问频率，期望收益计算公式可以改写为基于状态分布的形式，如下：
<span class="math display">\[
\begin{align*}
    \eta(\widetilde{\pi}) &amp;= \eta(\pi) + \sum_{t=0}^{\infty}
\sum_{s} P(s_{t}=s|\widetilde{\pi}) \sum_{a} \widetilde{\pi}(a|s)
\gamma^{t} A_{\pi}(s,a) \\
        &amp;= \eta(\pi) + \sum_{s} \sum_{t=0}^{\infty}
\gamma^{t}P(s_{t}=s|\widetilde{\pi}) \sum_{a} \widetilde{\pi}(a|s)
A_{\pi}(s,a) \\
        &amp;= \eta(\pi) + \sum_{s} \rho_{\color{red}\widetilde{\pi}}(s)
\sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a)
    \tag{3}
\end{align*}
\]</span></p>
<blockquote>
<p><span class="math inline">\(\sum_{a} \widetilde{\pi}(a|s)
A_{\pi}(s,a)\)</span>可以看作对策略<span
class="math inline">\(\pi\)</span>的优势函数<span
class="math inline">\(A_{\pi}\)</span>的加权求和，策略改进即调整一组更好的权重。</p>
</blockquote>
<p>从等式（3）可以看出，如果我们能够保证，从策略<span
class="math inline">\(\pi\)</span>更新到策略<span
class="math inline">\(\widetilde{\pi}\)</span>，对于每一个状态<span
class="math inline">\(s\)</span>，都有非负的期望优势，即<span
class="math inline">\(\sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a) \geq
0\)</span>，就能实现策略的单调改进。这个结果和使用确定性策略进行策略迭代的过程也具有一致性，策略迭代中策略改进采用<span
class="math inline">\(\widetilde{\pi}(s)=argmax_{a}A_{\pi}(s,a)\)</span>，如果存在<span
class="math inline">\((s,a)\)</span>使得<span
class="math inline">\({A_{\pi}(s,a)} \gt
0\)</span>，则策略将得到改善，否则，算法收敛到最优策略。</p>
<p><strong>第一次近似：</strong>等式(3)的计算依赖于<span
class="math inline">\(\rho_{\widetilde{\pi}}(s)\)</span>，这意味着需要用改进策略<span
class="math inline">\(\widetilde{\pi}\)</span>来采样（而不是用原策略<span
class="math inline">\(\pi\)</span>），用所有可能的改进策略<span
class="math inline">\(\widetilde{\pi}\)</span>采样并计算其期望收益是不现实的，因此论文做出如下近似（即忽略了策略变化对状态访问频率的影响）：
<span class="math display">\[
L_{\pi}(\widetilde{\pi}) = \eta(\pi) + \sum_{s} \rho_{\color{red}
\pi}(s) \sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a)
\]</span> 假设有参数化策略<span
class="math inline">\(\pi_{\theta}\)</span>，并且策略<span
class="math inline">\(\pi_{\theta}\)</span>关于参数<span
class="math inline">\(\theta\)</span>可微，上述近似函数<span
class="math inline">\(L_{\pi}\)</span>和原函数<span
class="math inline">\(\eta\)</span>一阶匹配。表示如下： <span
class="math display">\[
\begin{align*}
L_{\pi_{\theta_{0}}}(\pi_{\theta_{0}}) &amp;= \eta(\pi_{\theta_{0}}) \\
\nabla_{\theta} L_{\pi_{\theta_{0}}}(\pi_{\theta})|_{\theta=\theta_{0}}
&amp;= \nabla_{\theta} \eta(\pi_{\theta})|_{\theta=\theta_{0}}
\tag{4}
\end{align*}
\]</span> 上述结论表明，<strong>在策略变化较小的情况下，对<span
class="math inline">\(L\)</span>的改进也将导致<span
class="math inline">\(\eta\)</span>的改进</strong>。但该结论没有提供一个具体的步长设置的指导。</p>
<p>2002年，Kakade &amp;
Langford提出了一种策略更新方案叫做保守策略迭代（conservative policy
iteration），并且他们能够给出<span
class="math inline">\(\eta\)</span>改进的明确下限。假设<span
class="math inline">\(\pi_{old}\)</span>表示当前策略，<span
class="math inline">\(\pi^{\prime} = argmax_{\pi^{\prime}}
L_{\pi_{old}}(\pi^{\prime})\)</span>，改进策略<span
class="math inline">\(\pi_{new}\)</span>定义如下： <span
class="math display">\[
\pi_{new}(a|s) = (1-\alpha) \pi_{old}(a|s) + \alpha \pi^{\prime}(a|s)
\tag{5}
\]</span> 该方法给出<span class="math inline">\(\eta\)</span>下限如下：
<span class="math display">\[
\eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2 \epsilon
\gamma}{(1-\gamma)^2} \alpha^2 \tag{6} \\
\epsilon = \max_{s}|E_{a \sim \pi^{\prime}(a|s)}[A_{\pi}(s,a)]|
\]</span>
这个界限仅适用于由公式(5)生成的混合策略。这种策略在实际使用中笨重且受限，实际的策略更新方案最好适用于所有一般随机策略。</p>
<h2 id="一般随机策略的单调改进保证">一般随机策略的单调改进保证</h2>
<p>等式(6)表明，如果策略更新能够改进不等式右侧，就能够保证提高真实性能<span
class="math inline">\(\eta\)</span>。这篇文章的主要理论结果即，<strong>通过将<span
class="math inline">\(\alpha\)</span>替换为<span
class="math inline">\(\pi\)</span>和<span
class="math inline">\(\widetilde{\pi}\)</span>的距离，并且适当调整<span
class="math inline">\(\epsilon\)</span>，就可以将等式(6)的策略性能下限扩展到一般随机策略</strong>。本文采用全变差距离（total
variation divergence）对策略进行距离度量。</p>
<p><strong>全变差距离：</strong><span class="math inline">\(D_{TV}(p ||
q) = \frac{1}{2} \sum_{i} |p_{i} - q_{i}|\)</span>，其中<span
class="math inline">\(p,q\)</span>表示离散随机变量的分布律（结论可以通过用积分代替求和直接扩展到连续状态和动作场景）</p>
<p>定义策略<span class="math inline">\(\pi\)</span>和策略<span
class="math inline">\(\widetilde{\pi}\)</span>距离如下： <span
class="math display">\[
D_{TV}^{max}(\pi, \widetilde{\pi}) = \max_{s}
D_{TV}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s)) \tag{7}
\]</span> <strong>定理1：</strong>定义<span class="math inline">\(\alpha
= D_{TV}^{max}(\pi_{old}, \pi_{new})\)</span>，存在下面的策略性能下限：
<span class="math display">\[
\begin{align*}
    \eta(\pi_{new}) &amp;\ge L_{\pi_{old}}(\pi_{new}) - \frac{4 \epsilon
\gamma}{(1 - \gamma)^2} \alpha^2 \\
    \epsilon &amp;= \max_{s,a}|A_{\pi}(s,a)|
    \tag{8}
\end{align*}
\]</span></p>
<blockquote>
<p>证明：</p>
</blockquote>
<p>因为全变差距离和KL散度具有如下关系<span
class="math inline">\(D_{TV}(p || q)^2 \le D_{KL}(p ||
q)\)</span>，定义<span class="math inline">\(D_{KL}^{max}(\pi,
\widetilde{\pi})=max_{s}D_{KL}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s))\)</span>，可以得到如下策略性能下限：
<span class="math display">\[
\begin{align*}
    \eta(\widetilde{\pi}) &amp;\ge L_{\pi}(\widetilde{\pi}) -
CD_{KL}^{max}(\pi, \widetilde{\pi}) \\
    C &amp;= \frac{4 \epsilon \gamma}{(1 - \gamma)^2}
    \tag{9}
\end{align*}
\]</span> 定义<span class="math inline">\(M_{i}(\pi) = L_{\pi_{i}}(\pi)
- CD_{KL}^{max}(\pi_{i},\pi)\)</span>，于是有如下推导： <span
class="math display">\[
\begin{align*}
    \eta(\pi_{i+1}) &amp;\ge M_{i}(\pi_{i+1}) \\
    \eta(\pi_{i}) &amp;= M_{i}(\pi_{i}) \\
    \eta(\pi_{i+1}) - \eta(\pi_{i}) &amp;\ge M_{i}(\pi_{i+1}) -
M_{i}(\pi_{i})
    \tag{10}
\end{align*}
\]</span> 由此可以得到，<strong>通过在每轮迭代时最大化<span
class="math inline">\(M_{i}(\pi_{i+1})\)</span>，算法能够保证真实策略性能<span
class="math inline">\(\eta\)</span>单调提升</strong>。这篇文章基于此提出了一个近似策略迭代方案（TRPO算法是对该方案的近似），如下图：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20250104195614428.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="参数化策略的优化">参数化策略的优化</h2>
<p>前面考虑问题时没有深入到策略参数层面。现在考虑参数化策略<span
class="math inline">\(\pi_{\theta}(a|s)\)</span>，并使用<span
class="math inline">\(\theta\)</span>重写前面的符号，例如<span
class="math inline">\(\eta(\theta) := \eta(\pi_{\theta})\)</span>,<span
class="math inline">\(L_{\theta}(\widetilde{\theta}) :=
L_{\pi_{\theta}}(\pi_{\widetilde{\theta}})\)</span>。</p>
<p>前面的章节已经表明，<span class="math inline">\(\eta(\theta) \ge
L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old},
\theta)\)</span>，在<span class="math inline">\(\theta =
\theta_{old}\)</span>时不等式取等号，为了保证策略性能<span
class="math inline">\(\eta\)</span>单调改进，优化目标为<span
class="math inline">\(\max_{\theta} [L_{\theta_{old}}(\theta)
-CD_{KL}^{max}(\theta_{old}, \theta)]\)</span>。</p>
<p>实际上，<strong>如果我们使用上述理论推荐的惩罚系数<span
class="math inline">\(C\)</span>，步长将会非常小。一种方式是针对新旧策略的<span
class="math inline">\(KL\)</span>散度施加约束，即一个信任区域</strong>：
<span class="math display">\[
\begin{align*}
    &amp; \max_{\theta} L_{\theta_{old}}(\theta) \\
    &amp; s.t.D_{KL}^{max}(\theta_{old}, \theta) \le \delta
    \tag{11}
\end{align*}
\]</span> <strong>第二次近似：</strong>上面约束计算新旧参数的<span
class="math inline">\(KL\)</span>散度是针对所有状态<span
class="math inline">\(s\)</span>求<span
class="math inline">\(max\)</span>，因此虽然上式是由理论推导得到的，但在实际中直接优化这个问题是非常困难的。因此论文使用平均<span
class="math inline">\(KL\)</span>散度替代<span
class="math inline">\(max\)</span>操作，即： <span
class="math display">\[
\bar{D}_{KL}^{\rho}(\theta_{1},\theta_{2}) := E_{\color{red} s \sim
\rho_{\pi_{\theta_{1}}}}[D_{KL}(\pi_{\theta_{1}}(\cdot|s) ||
\pi_{\theta_{2}}(\cdot|s))]
\]</span> 因此优化问题变为： <span class="math display">\[
\begin{align*}
    \max_{\theta} \space &amp; L_{\theta_{old}}(\theta) \\
    s.t. \space &amp; \bar{D}_{KL}^{\rho_{\pi_{old}}}(\theta_{old},
\theta) \le \delta
    \tag{12}
\end{align*}
\]</span></p>
<h2 id="基于样本估计目标和约束">基于样本估计目标和约束</h2>
<p>现在考虑<strong>使用蒙特卡洛模拟，用采样样本近似目标函数和约束函数</strong>。目标是求解下面的优化问题（对<span
class="math inline">\(L_{\theta_{old}}\)</span>展开，其中<span
class="math inline">\(\eta(\theta_{old})\)</span>相对于优化变量<span
class="math inline">\(\theta\)</span>为常数，直接舍去）： <span
class="math display">\[
\begin{align*}
    \max_{\theta} \space &amp; \sum_{s} \rho_{\theta_{old}}(s) \sum_{a}
\pi_{\theta}(a|s) A_{\theta_{old}}(s,a) \\
    s.t. \space &amp; \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},
\theta) \le \delta
    \tag{13}
\end{align*}
\]</span> 对上面的优化问题做3次替换：</p>
<ol type="1">
<li>将<span
class="math inline">\(\sum_{s}\rho_{\theta_{old}}(s)[...]\)</span>替换为<span
class="math inline">\(E_{s \sim
\rho_{\theta_{old}}}[...]\)</span>（原文中还有<span
class="math inline">\(\frac{1}{1 -
\gamma}\)</span>系数，一方面我没看懂怎么推出来的，另一方面这个系数不影响优化问题本身，因此忽略）</li>
<li>将<span class="math inline">\(A_{\theta_{old}}\)</span>替换为<span
class="math inline">\(Q_{\theta_{old}}\)</span></li>
<li>将关于动作<span class="math inline">\(a\)</span>的求和<span
class="math inline">\(\sum_a\)</span>替换为一个重要度采样估计量：<span
class="math inline">\(\sum_{a} \pi_{\theta}(a|s_{n})
A_{\theta_{old}}(s_{n},a) = E_{a \sim
q}[\frac{\pi_{\theta}(a|s_{n})}{q(a|s_{n})}
A_{\theta_{old}}(s_{n},a)]\)</span></li>
</ol>
<p>于是优化问题变为如下形式： <span class="math display">\[
\begin{align*}
    \max_{\theta} \space &amp; E_{s \sim \rho_{\theta_{old}}, a \sim
q}[\frac{\pi_{\theta}(a|s)}{q(a|s)}Q_{\theta_{old}}(s,a)] \\
    s.t. \space &amp; E_{s \sim
\rho_{\theta_{old}}}[D_{KL}(\pi_{\theta_{old}}(\cdot|s) ||
\pi_{\theta}(\cdot|s))] \le \delta
    \tag{14}
\end{align*}
\]</span> 下面介绍两种采样方案——single path和vine</p>
<h3 id="single-path">Single Path</h3>
<p>基于<span class="math inline">\(s_{0} \sim
\rho_{0}\)</span>分布，收集一系列状态序列，模拟<span
class="math inline">\(\pi_{\theta_{old}}\)</span>策略执行若干时间步，生成轨迹<span
class="math inline">\(s_{0},a_{0},s_{1},a_{1},...,s_{T-1},a_{T-1},s_{T}\)</span>。这种方案下，<span
class="math inline">\(q(a|s)=\pi_{\theta_{old}}(a|s)\)</span>。</p>
<h3 id="vine">Vine</h3>
<p>基于<span class="math inline">\(s_{0} \sim
\rho_{0}\)</span>分布，收集一系列状态序列，模拟<span
class="math inline">\(\pi_{\theta_{i}}\)</span>策略生成轨迹。然后沿着这些轨迹，选择关于<span
class="math inline">\(N\)</span>个状态的子集，表示为<span
class="math inline">\(s_{1},s_{2},...,s_{N}\)</span></p>
<h2 id="qa">Q&amp;A</h2>
<ol type="1">
<li><p>什么是自然梯度策略（natural policy gradient）？</p></li>
<li><p>什么是无导数优化方法（derivative-free optimization
methods）？启发式搜索算法？</p></li>
<li><p>状态访问频率为什么取折扣？是为了配合后续推导？是否有单独的含义？</p></li>
<li><p>minorization-maximization (MM) algorithm定义？proximal gradient
methods定义？mirror descent定义？</p></li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/MARL/">MARL</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/MARL/">MARL</a>
                    
                      <a class="hover-with-bg" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/11/12/GRF/">
                        <span class="hidden-mobile">GRF环境配置</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  

  

  
    <!-- Google gtag.js -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-HB7P267418"></script>
    <script defer>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-HB7P267418');
    </script>
  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
