<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>The Dormant Neuron Phenomenon in Deep Reinforcement Learning</title>
    <link href="/2025/02/18/The_Dormant_Neuron_Phenomenon_in_Deep_Reinforcement_Learning/"/>
    <url>/2025/02/18/The_Dormant_Neuron_Phenomenon_in_Deep_Reinforcement_Learning/</url>
    
    <content type="html"><![CDATA[<h1id="the-dormant-neuron-phenomenon-in-deep-reinforcement-learning">TheDormant Neuron Phenomenon in Deep Reinforcement Learning</h1><p>原文：<a href="https://arxiv.org/abs/2302.12902">The Dormant NeuronPhenomenon in Deep Reinforcement Learning</a></p><blockquote><p>当前的训练技术，并不能够充分发挥网络的能力。</p><p>这篇工作介绍了DRL领域的一种现象——<strong>模型随着训练丧失拟合能力</strong>。</p><p>这同<strong>休眠神经元现象(dormant neuronphenomenon)</strong>存在相关性。</p><p>这篇工作揭示了这种现象在DRL领域普遍存在；分析了产生这种现象的可能原因；以及其造成的负面影响。</p><p>提出了ReDo算法缓解问题。</p><p>验证了ReDo算法的有效性；可以在提高回放率时，缓解产生的负面影响；可以在提高模型复杂度时，帮助模型更好的发挥潜力</p><p><em>这篇文章关注的是价值网络的问题（实验基于value-base算法）</em></p></blockquote><h2 id="前置知识">前置知识</h2><p>RL中引入DL，一方面提升了算法性能，但引入了新的训练问题。针对这些问题，一方面，改进RL算法（经验回放池，目标网络），另一方面，分析网络模型在RL场景下的特性。</p><p>监督学习中的“scalinglaw”表明模型表现和参数数量具有正相关性。在RL中，有证据表明，<strong>即使参数量足够的情况下，随着训练的进行，模型失去拟合能力，不能适应新的目标</strong>。</p><p>和监督学习相比，RL很重要的一点不同在于，<strong>网络训练过程的非平稳</strong>：1)<strong>输入数据非平稳</strong>。随着策略的演化，数据分布在变动；2)<strong>拟合目标非平稳</strong>。目标网络周期性更新。</p><blockquote><p>这种非平稳叠加上自举，是否还能收敛到最优价值函数？本身TD算法就是有偏估计。</p><p>此外，基于函数估计的RL还要考虑一个问题，就是函数参数更新的耦合性，不同于基于表格的RL，函数参数更新会对其他状态的价值产生影响。</p><p>将DRL的价值函数估计同DL的回归任务做比较，DRL的价值函数是否更锐利。例如房价预测，特征的变化对回归值影响相对平滑，DRL的价值函数是否有这种特性，或者说，这是否就是不同游戏环境+reward设计的一般性区别？</p></blockquote><p><strong>定义1（休眠神经元）：</strong>给定输入分布下，计算每个神经元的激活值的期望。然后，计算每层神经元期望激活值的平均数，每个神经元除以自己所在层的平均数得到<strong>休眠度</strong></p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218190840740.png" /></p><blockquote><p>休眠神经元的定义是依赖于输入的分布的，在某个分布下休眠的神经元，在另一个分布下，未必还是休眠的，反之亦然。而输入分布随价值网络变动。</p><p>虽然给出了休眠神经元的定义，但在后续实验中，具体是如何计算的，输入的分布是如何取的，论文并没有提及。</p></blockquote><p><strong>定义2（休眠神经元现象）：</strong>在训练过程中，休眠神经元的数量稳定上升。</p><h2 id="实验分析休眠神经元现象">实验分析休眠神经元现象</h2><h3 id="休眠神经元现象广泛存在">休眠神经元现象广泛存在</h3><p><strong>实验设置：</strong>DQN + Arcade Learning Environment</p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218193237390.png" /></p><h3 id="拟合目标非平稳加剧现象">拟合目标非平稳加剧现象</h3><p><strong>实验设置：</strong>CIFAR-10任务，1)固定标签，训练一个卷积神经网络；2)每20个epoch打乱标签，训练一个卷积神经网络。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218194219614.png" /></p><blockquote><p>CIFAR-10是分类任务，RL中价值估计更接近回归任务，换一个回归任务分析更合理</p></blockquote><h3 id="输入数据非平稳不是主要原因">输入数据非平稳不是主要原因</h3><p><strong>实验设置：</strong>离线RL（数据集固定） + DQN + ArcadeLearning Environment</p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218194836137.png" /></p><blockquote><p>原文提到，为了分析这种设定下休眠神经元的来源，进行了fixed randomtargets实验（即将输入数据和拟合目标都固定，图中蓝色曲线），因为蓝色曲线休眠神经元更少，所以支撑了结论——拟合目标非平稳是休眠神经元现象的主要来源。但奇怪的是，蓝色曲线依然随训练过程上升，跟监督学习的下降趋势并不一致，也就是说，固定拟合目标后，还有其他原因，导致了神经元的休眠。</p></blockquote><h3 id="休眠神经元保持休眠">休眠神经元保持休眠</h3><p><strong>实验设置：</strong>DQN + Arcade Learning Environment</p><p><strong>定义3（重叠系数）：</strong>集合<spanclass="math inline">\(X\)</span>和集合<spanclass="math inline">\(Y\)</span>的重叠系数为<spanclass="math inline">\(overlap(X,Y)=\frac{|X \capY|}{\min(|X|,|Y|)}\)</span></p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218201344268.png" /></p><blockquote><p>Figure 5 plots the overlap coefficient between the set of dormantneurons in the penultimate layer at the current iteration, and thehistorical set of dormant neurons.</p><p>作者认为图中的增长强力的支撑了结论——休眠神经元在训练的剩余时间内保持休眠。但图5中最高重叠率是80%，意味着还是有20%的休眠神经元能够通过训练，或输入数据分布的改变等原因重新激活。</p></blockquote><h3 id="频繁的梯度更新加剧现象">频繁的梯度更新加剧现象</h3><p><strong>实验设置：</strong>DQN + Arcade Learning Environment</p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218202613383.png" /></p><h3 id="休眠神经元阻碍学习新任务">休眠神经元阻碍学习新任务</h3><p><strong>实验设置：</strong>在DemonAttack环境下，训练一个DQN智能体，设定RR=1，如图7所示，网络中存在很多休眠神经元，后续用“预训练网络”指代这个网络。然后找一个表现良好的DQN网络作为teacher网络，预训练网络作为student网络，进行知识蒸馏，同时用一个随机初始化的网络作为student网络也进行知识蒸馏。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218203923741.png" /></p><blockquote><p>知识蒸馏的过程，输入数据的分布是什么？图8中预训练模型loss甚至在上升，难道不是应当先保证loss下降或保持不变，再进行现象分析？</p><p>知识蒸馏的过程，对于student网络来说本身就是一个监督学习，会什么还会出现休眠神经元比例的上升？</p></blockquote><h2 id="redo算法">ReDo算法</h2><h3 id="算法介绍">算法介绍</h3><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218204622549.png" /></p><h3 id="算法的有效性">算法的有效性</h3><h4id="减少休眠神经元提升智能体表现">减少休眠神经元，提升智能体表现</h4><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218205017914.png" /></p><blockquote><p>这个任务是否具有代表性？换一个游戏，实验结果还会是这样吗？</p></blockquote><h4 id="效果优于其他正则化方法">效果优于其他正则化方法</h4><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218205417289.png" /></p><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218205448015.png" /></p><blockquote><p>为什么在这种MuJoCo+SAC场景下，这些改进策略不再能够提升算法性能，难道休眠神经元在这类游戏场景中比较少？</p></blockquote><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218205503904.png" /></p><h3 id="缓解提高回放率产生的负面影响">缓解提高回放率产生的负面影响</h3><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218210236161.png" /></p><blockquote><p>在多种场景（不同multi-step，不同网络结构，不同RL算法）下ReDo都能起到缓解，提高回放率产生的负面影响。</p></blockquote><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218211736207.png" /></p><h3 id="帮助模型更好的发挥潜力">帮助模型更好的发挥潜力</h3><p><imgsrc="https://cdn.jsdelivr.net/gh/archi-z/image_hosting//SIGS/image-20250218212141513.png" /></p><blockquote><p>简单增加模型参数量，依靠传统训练，复杂模型的拟合能力发挥不出来；通过ReDo，休眠神经元的数量能够保持在一种较低的水平，增加模型参数量，算法表现还能得到一定程度提升。</p><p>不同模型宽度，休眠神经元的比例变化趋势非常接近，如何解释这个现象？</p></blockquote><h2 id="评论">评论</h2><ul><li><p>有关神经元休眠的原因似乎还没调查清楚，文章针对问题打了个补丁，提升了算法性能。有进一步定位的空间。</p></li><li><p>问题不在于休眠神经元数量上升，而在于完美拟合该游戏的真实价值函数需要70%的神经元，却有50%的神经元陷入休眠，导致模型拟合能力发挥不出来。如果仅有30%神经元休眠，且模型表现优异，理论上应该是正常的，预示着过拟合。如果仅有20%神经元休眠，可能也意味着欠拟合。可以以高阶多项式拟合为例。</p></li><li><p>理论上在RL领域不存在监督学习的那种过拟合问题，因为给定了环境的模型（游戏本身），RL的数据可以无限生成，但是生成数据本身依赖于策略，RL因此有特有的问题——探索&amp;利用。</p></li><li><p>可以做一个小测试。调整epsilon衰减的速率，观察休眠神经元变化的趋势是否会受影响？</p></li><li><p>相较于论文中经常出现的休眠神经元的比例，可以尝试去统计神经元激活值的分布，可能更有助于我们发现问题。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DRL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>torch节省显存方法</title>
    <link href="/2025/01/12/torch%E8%8A%82%E7%9C%81%E6%98%BE%E5%AD%98%E6%96%B9%E6%B3%95/"/>
    <url>/2025/01/12/torch%E8%8A%82%E7%9C%81%E6%98%BE%E5%AD%98%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="torch节省显存方法">torch节省显存方法</h1><h2 id="手动删除变量">手动删除变量</h2><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">del</span> var<br>torch.cuda.empty_cache()<br></code></pre></div></td></tr></table></figure><h2 id="禁止自动梯度计算">禁止自动梯度计算</h2><blockquote><p>自动梯度计算似乎还挺占显存的，不需要的情况下最好关掉</p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.no_grad():<br>    tensor1 = tensor2 @ tensor3<br></code></pre></div></td></tr></table></figure><h2 id="降低数据精度">降低数据精度</h2><blockquote><p>修改精度时不要用tensor.float()，tensor.int()这种语法，因为不清楚具体的数据类型，例如tensor.int()可能会将数据转成torch.int32，这跟torch.float32占用的存储空间是一样大的，起不到任何效果。</p><p>相反，应该使用tensor.to()具体指定数据格式，例如tensor.to(torch.int8)，tensor.to(torch.float16)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">tensor1 = torch.zeros((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), dtype=torch.float32)<span class="hljs-comment"># 新建数据时，使用torch.float16,torch.int8降低精度</span><br>tensor1 = tensor1.half() <span class="hljs-comment"># 使用torch.float16替代torch.float32</span><br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工程技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>torch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TRPO论文笔记</title>
    <link href="/2025/01/09/TRPO/"/>
    <url>/2025/01/09/TRPO/</url>
    
    <content type="html"><![CDATA[<h1 id="trpo论文笔记">TRPO论文笔记</h1><blockquote><p>摘要：一种策略改进过程，<strong>保证单调改进</strong>。通过对理论进行几次近似，开发了TRPO算法。该算法类似于自然梯度策略（naturalpolicygradient）方法。尽管近似值与理论有所偏差，但它往往能够实现单调改进，几乎不用调超参。</p></blockquote><h2 id="背景">背景</h2><p>大多数策略优化算法可以分为三类：1）策略迭代方法（policy iterationmethods），即基于值函数的方法；2）策略梯度方法（policy gradientmethods），即参数化策略的方法；3）无导数优化方法（derivative-freeoptimization methods）。</p><p>无导数优化方法在很多问题上受到青睐，因为它们效果好，容易理解和实现。ADP（approximatedynamicprogramming）和基于梯度的方法无法击败无导数优化方法，这是令人不满意的，因为基于梯度的优化算法比无梯度方法享有更好的样本复杂度保证。基于梯度的优化方法在监督学习的函数估计中已经比较成功，如何将其扩展到强化学习？</p><p>文章证明<strong>最小化某个替代目标函数可以保证策略改进</strong>。通过对理论算法进行一系列近似，文章给出了TRPO算法。文章给出了这个算法的两种变体——singlepath和vine。前者可以应用于model-free场景，后者需要将系统恢复到特定状态（这通常只能在模拟中实现）。</p><h2 id="预备知识">预备知识</h2><p>基于策略<span class="math inline">\(\pi\)</span>的期望收益<spanclass="math inline">\(\eta(\pi)\)</span>和优势函数<spanclass="math inline">\(A_{\pi}(s_{t},a_{t})\)</span>，计算策略<spanclass="math inline">\(\widetilde{\pi}\)</span>的期望收益<spanclass="math inline">\(\eta(\widetilde{\pi})\)</span>，公式如下： <spanclass="math display">\[\eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,...\sim\color{red}\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\color{red}\pi}(s_{t},a_{t})]\tag{1}\]</span></p><blockquote><p>证明：</p></blockquote><p>定义策略<span class="math inline">\(\pi\)</span>下状态<spanclass="math inline">\(s\)</span>的<strong>折扣状态访问频率（discountedvisitation frequencies）</strong>为<spanclass="math inline">\(\rho_{\pi}(s)\)</span>如下： <spanclass="math display">\[\rho_{\pi}(s)=P(s_{0}=s)+\gamma P(s_{1}=s)+\gamma^{2}P(s_{2}=s)+\dots\tag{2}\]</span></p><blockquote><p>上面的折扣状态访问频率未归一化，归一化形式为<spanclass="math inline">\(\rho_{\pi}(s) = (1 - \gamma) \sum_{t=0}^{\infty}\gamma^{t} P(s_{t}=s)\)</span></p></blockquote><p>基于折扣状态访问频率，期望收益计算公式可以改写为基于状态分布的形式，如下：<span class="math display">\[\begin{align*}    \eta(\widetilde{\pi}) &amp;= \eta(\pi) + \sum_{t=0}^{\infty}\sum_{s} P(s_{t}=s|\widetilde{\pi}) \sum_{a} \widetilde{\pi}(a|s)\gamma^{t} A_{\pi}(s,a) \\        &amp;= \eta(\pi) + \sum_{s} \sum_{t=0}^{\infty}\gamma^{t}P(s_{t}=s|\widetilde{\pi}) \sum_{a} \widetilde{\pi}(a|s)A_{\pi}(s,a) \\        &amp;= \eta(\pi) + \sum_{s} \rho_{\color{red}\widetilde{\pi}}(s)\sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a)    \tag{3}\end{align*}\]</span></p><blockquote><p><span class="math inline">\(\sum_{a} \widetilde{\pi}(a|s)A_{\pi}(s,a)\)</span>可以看作对策略<spanclass="math inline">\(\pi\)</span>的优势函数<spanclass="math inline">\(A_{\pi}\)</span>的加权求和，策略改进即调整一组更好的权重。</p></blockquote><p>从等式（3）可以看出，如果我们能够保证，从策略<spanclass="math inline">\(\pi\)</span>更新到策略<spanclass="math inline">\(\widetilde{\pi}\)</span>，对于每一个状态<spanclass="math inline">\(s\)</span>，都有非负的期望优势，即<spanclass="math inline">\(\sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a) \geq0\)</span>，就能实现策略的单调改进。这个结果和使用确定性策略进行策略迭代的过程也具有一致性，策略迭代中策略改进采用<spanclass="math inline">\(\widetilde{\pi}(s)=argmax_{a}A_{\pi}(s,a)\)</span>，如果存在<spanclass="math inline">\((s,a)\)</span>使得<spanclass="math inline">\({A_{\pi}(s,a)} \gt0\)</span>，则策略将得到改善，否则，算法收敛到最优策略。</p><p><strong>第一次近似：</strong>等式(3)的计算依赖于<spanclass="math inline">\(\rho_{\widetilde{\pi}}(s)\)</span>，这意味着需要用改进策略<spanclass="math inline">\(\widetilde{\pi}\)</span>来采样（而不是用原策略<spanclass="math inline">\(\pi\)</span>），用所有可能的改进策略<spanclass="math inline">\(\widetilde{\pi}\)</span>采样并计算其期望收益是不现实的，因此论文做出如下近似（即忽略了策略变化对状态访问频率的影响）：<span class="math display">\[L_{\pi}(\widetilde{\pi}) = \eta(\pi) + \sum_{s} \rho_{\color{red}\pi}(s) \sum_{a} \widetilde{\pi}(a|s) A_{\pi}(s,a)\]</span> 假设有参数化策略<spanclass="math inline">\(\pi_{\theta}\)</span>，并且策略<spanclass="math inline">\(\pi_{\theta}\)</span>关于参数<spanclass="math inline">\(\theta\)</span>可微，上述近似函数<spanclass="math inline">\(L_{\pi}\)</span>和原函数<spanclass="math inline">\(\eta\)</span>一阶匹配。表示如下： <spanclass="math display">\[\begin{align*}L_{\pi_{\theta_{0}}}(\pi_{\theta_{0}}) &amp;= \eta(\pi_{\theta_{0}}) \\\nabla_{\theta} L_{\pi_{\theta_{0}}}(\pi_{\theta})|_{\theta=\theta_{0}}&amp;= \nabla_{\theta} \eta(\pi_{\theta})|_{\theta=\theta_{0}}\tag{4}\end{align*}\]</span> 上述结论表明，<strong>在策略变化较小的情况下，对<spanclass="math inline">\(L\)</span>的改进也将导致<spanclass="math inline">\(\eta\)</span>的改进</strong>。但该结论没有提供一个具体的步长设置的指导。</p><p>2002年，Kakade &amp;Langford提出了一种策略更新方案叫做保守策略迭代（conservative policyiteration），并且他们能够给出<spanclass="math inline">\(\eta\)</span>改进的明确下限。假设<spanclass="math inline">\(\pi_{old}\)</span>表示当前策略，<spanclass="math inline">\(\pi^{\prime} = argmax_{\pi^{\prime}}L_{\pi_{old}}(\pi^{\prime})\)</span>，改进策略<spanclass="math inline">\(\pi_{new}\)</span>定义如下： <spanclass="math display">\[\pi_{new}(a|s) = (1-\alpha) \pi_{old}(a|s) + \alpha \pi^{\prime}(a|s)\tag{5}\]</span> 该方法给出<span class="math inline">\(\eta\)</span>下限如下：<span class="math display">\[\eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2 \epsilon\gamma}{(1-\gamma)^2} \alpha^2 \tag{6} \\\epsilon = \max_{s}|E_{a \sim \pi^{\prime}(a|s)}[A_{\pi}(s,a)]|\]</span>这个界限仅适用于由公式(5)生成的混合策略。这种策略在实际使用中笨重且受限，实际的策略更新方案最好适用于所有一般随机策略。</p><h2 id="一般随机策略的单调改进保证">一般随机策略的单调改进保证</h2><p>等式(6)表明，如果策略更新能够改进不等式右侧，就能够保证提高真实性能<spanclass="math inline">\(\eta\)</span>。这篇文章的主要理论结果即，<strong>通过将<spanclass="math inline">\(\alpha\)</span>替换为<spanclass="math inline">\(\pi\)</span>和<spanclass="math inline">\(\widetilde{\pi}\)</span>的距离，并且适当调整<spanclass="math inline">\(\epsilon\)</span>，就可以将等式(6)的策略性能下限扩展到一般随机策略</strong>。本文采用全变差距离（totalvariation divergence）对策略进行距离度量。</p><p><strong>全变差距离：</strong><span class="math inline">\(D_{TV}(p ||q) = \frac{1}{2} \sum_{i} |p_{i} - q_{i}|\)</span>，其中<spanclass="math inline">\(p,q\)</span>表示离散随机变量的分布律（结论可以通过用积分代替求和直接扩展到连续状态和动作场景）</p><p>定义策略<span class="math inline">\(\pi\)</span>和策略<spanclass="math inline">\(\widetilde{\pi}\)</span>距离如下： <spanclass="math display">\[D_{TV}^{max}(\pi, \widetilde{\pi}) = \max_{s}D_{TV}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s)) \tag{7}\]</span> <strong>定理1：</strong>定义<span class="math inline">\(\alpha= D_{TV}^{max}(\pi_{old}, \pi_{new})\)</span>，存在下面的策略性能下限：<span class="math display">\[\begin{align*}    \eta(\pi_{new}) &amp;\ge L_{\pi_{old}}(\pi_{new}) - \frac{4 \epsilon\gamma}{(1 - \gamma)^2} \alpha^2 \\    \epsilon &amp;= \max_{s,a}|A_{\pi}(s,a)|    \tag{8}\end{align*}\]</span></p><blockquote><p>证明：</p></blockquote><p>因为全变差距离和KL散度具有如下关系<spanclass="math inline">\(D_{TV}(p || q)^2 \le D_{KL}(p ||q)\)</span>，定义<span class="math inline">\(D_{KL}^{max}(\pi,\widetilde{\pi})=max_{s}D_{KL}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s))\)</span>，可以得到如下策略性能下限：<span class="math display">\[\begin{align*}    \eta(\widetilde{\pi}) &amp;\ge L_{\pi}(\widetilde{\pi}) -CD_{KL}^{max}(\pi, \widetilde{\pi}) \\    C &amp;= \frac{4 \epsilon \gamma}{(1 - \gamma)^2}    \tag{9}\end{align*}\]</span> 定义<span class="math inline">\(M_{i}(\pi) = L_{\pi_{i}}(\pi)- CD_{KL}^{max}(\pi_{i},\pi)\)</span>，于是有如下推导： <spanclass="math display">\[\begin{align*}    \eta(\pi_{i+1}) &amp;\ge M_{i}(\pi_{i+1}) \\    \eta(\pi_{i}) &amp;= M_{i}(\pi_{i}) \\    \eta(\pi_{i+1}) - \eta(\pi_{i}) &amp;\ge M_{i}(\pi_{i+1}) -M_{i}(\pi_{i})    \tag{10}\end{align*}\]</span> 由此可以得到，<strong>通过在每轮迭代时最大化<spanclass="math inline">\(M_{i}(\pi_{i+1})\)</span>，算法能够保证真实策略性能<spanclass="math inline">\(\eta\)</span>单调提升</strong>。这篇文章基于此提出了一个近似策略迭代方案（TRPO算法是对该方案的近似），如下图：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20250104195614428.png" /></p><h2 id="参数化策略的优化">参数化策略的优化</h2><p>前面考虑问题时没有深入到策略参数层面。现在考虑参数化策略<spanclass="math inline">\(\pi_{\theta}(a|s)\)</span>，并使用<spanclass="math inline">\(\theta\)</span>重写前面的符号，例如<spanclass="math inline">\(\eta(\theta) := \eta(\pi_{\theta})\)</span>,<spanclass="math inline">\(L_{\theta}(\widetilde{\theta}) :=L_{\pi_{\theta}}(\pi_{\widetilde{\theta}})\)</span>。</p><p>前面的章节已经表明，<span class="math inline">\(\eta(\theta) \geL_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old},\theta)\)</span>，在<span class="math inline">\(\theta =\theta_{old}\)</span>时不等式取等号，为了保证策略性能<spanclass="math inline">\(\eta\)</span>单调改进，优化目标为<spanclass="math inline">\(\max_{\theta} [L_{\theta_{old}}(\theta)-CD_{KL}^{max}(\theta_{old}, \theta)]\)</span>。</p><p>实际上，<strong>如果我们使用上述理论推荐的惩罚系数<spanclass="math inline">\(C\)</span>，步长将会非常小。一种方式是针对新旧策略的<spanclass="math inline">\(KL\)</span>散度施加约束，即一个信任区域</strong>：<span class="math display">\[\begin{align*}    &amp; \max_{\theta} L_{\theta_{old}}(\theta) \\    &amp; s.t.D_{KL}^{max}(\theta_{old}, \theta) \le \delta    \tag{11}\end{align*}\]</span> <strong>第二次近似：</strong>上面约束计算新旧参数的<spanclass="math inline">\(KL\)</span>散度是针对所有状态<spanclass="math inline">\(s\)</span>求<spanclass="math inline">\(max\)</span>，因此虽然上式是由理论推导得到的，但在实际中直接优化这个问题是非常困难的。因此论文使用平均<spanclass="math inline">\(KL\)</span>散度替代<spanclass="math inline">\(max\)</span>操作，即： <spanclass="math display">\[\bar{D}_{KL}^{\rho}(\theta_{1},\theta_{2}) := E_{\color{red} s \sim\rho_{\pi_{\theta_{1}}}}[D_{KL}(\pi_{\theta_{1}}(\cdot|s) ||\pi_{\theta_{2}}(\cdot|s))]\]</span> 因此优化问题变为： <span class="math display">\[\begin{align*}    \max_{\theta} \space &amp; L_{\theta_{old}}(\theta) \\    s.t. \space &amp; \bar{D}_{KL}^{\rho_{\pi_{old}}}(\theta_{old},\theta) \le \delta    \tag{12}\end{align*}\]</span></p><h2 id="基于样本估计目标和约束">基于样本估计目标和约束</h2><p>现在考虑<strong>使用蒙特卡洛模拟，用采样样本近似目标函数和约束函数</strong>。目标是求解下面的优化问题（对<spanclass="math inline">\(L_{\theta_{old}}\)</span>展开，其中<spanclass="math inline">\(\eta(\theta_{old})\)</span>相对于优化变量<spanclass="math inline">\(\theta\)</span>为常数，直接舍去）： <spanclass="math display">\[\begin{align*}    \max_{\theta} \space &amp; \sum_{s} \rho_{\theta_{old}}(s) \sum_{a}\pi_{\theta}(a|s) A_{\theta_{old}}(s,a) \\    s.t. \space &amp; \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta    \tag{13}\end{align*}\]</span> 对上面的优化问题做3次替换：</p><ol type="1"><li>将<spanclass="math inline">\(\sum_{s}\rho_{\theta_{old}}(s)[...]\)</span>替换为<spanclass="math inline">\(E_{s \sim\rho_{\theta_{old}}}[...]\)</span>（原文中还有<spanclass="math inline">\(\frac{1}{1 -\gamma}\)</span>系数，一方面我没看懂怎么推出来的，另一方面这个系数不影响优化问题本身，因此忽略）</li><li>将<span class="math inline">\(A_{\theta_{old}}\)</span>替换为<spanclass="math inline">\(Q_{\theta_{old}}\)</span></li><li>将关于动作<span class="math inline">\(a\)</span>的求和<spanclass="math inline">\(\sum_a\)</span>替换为一个重要度采样估计量：<spanclass="math inline">\(\sum_{a} \pi_{\theta}(a|s_{n})A_{\theta_{old}}(s_{n},a) = E_{a \simq}[\frac{\pi_{\theta}(a|s_{n})}{q(a|s_{n})}A_{\theta_{old}}(s_{n},a)]\)</span></li></ol><p>于是优化问题变为如下形式： <span class="math display">\[\begin{align*}    \max_{\theta} \space &amp; E_{s \sim \rho_{\theta_{old}}, a \simq}[\frac{\pi_{\theta}(a|s)}{q(a|s)}Q_{\theta_{old}}(s,a)] \\    s.t. \space &amp; E_{s \sim\rho_{\theta_{old}}}[D_{KL}(\pi_{\theta_{old}}(\cdot|s) ||\pi_{\theta}(\cdot|s))] \le \delta    \tag{14}\end{align*}\]</span> 下面介绍两种采样方案——single path和vine</p><h3 id="single-path">Single Path</h3><p>基于<span class="math inline">\(s_{0} \sim\rho_{0}\)</span>分布，收集一系列状态序列，模拟<spanclass="math inline">\(\pi_{\theta_{old}}\)</span>策略执行若干时间步，生成轨迹<spanclass="math inline">\(s_{0},a_{0},s_{1},a_{1},...,s_{T-1},a_{T-1},s_{T}\)</span>。这种方案下，<spanclass="math inline">\(q(a|s)=\pi_{\theta_{old}}(a|s)\)</span>。</p><h3 id="vine">Vine</h3><p>基于<span class="math inline">\(s_{0} \sim\rho_{0}\)</span>分布，收集一系列状态序列，模拟<spanclass="math inline">\(\pi_{\theta_{i}}\)</span>策略生成轨迹。然后沿着这些轨迹，选择关于<spanclass="math inline">\(N\)</span>个状态的子集，表示为<spanclass="math inline">\(s_{1},s_{2},...,s_{N}\)</span></p><h2 id="qa">Q&amp;A</h2><ol type="1"><li><p>什么是自然梯度策略（natural policy gradient）？</p></li><li><p>什么是无导数优化方法（derivative-free optimizationmethods）？启发式搜索算法？</p></li><li><p>状态访问频率为什么取折扣？是为了配合后续推导？是否有单独的含义？</p></li><li><p>minorization-maximization (MM) algorithm定义？proximal gradientmethods定义？mirror descent定义？</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>MARL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MARL</tag>
      
      <tag>论文笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GRF环境配置</title>
    <link href="/2024/11/12/GRF/"/>
    <url>/2024/11/12/GRF/</url>
    
    <content type="html"><![CDATA[<h1 id="grf环境配置">GRF环境配置</h1><h2 id="源代码仓">源代码仓</h2><blockquote><p>https://github.com/google-research/football</p></blockquote><h2 id="安装依赖">安装依赖</h2><figure class="highlight q"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs q">sudo apt-<span class="hljs-built_in">get</span> install git cmake build-essential libgl1-mesa-<span class="hljs-built_in">dev</span> libsdl2-<span class="hljs-built_in">dev</span> \<br>libsdl2-image-<span class="hljs-built_in">dev</span> libsdl2-ttf-<span class="hljs-built_in">dev</span> libsdl2-gfx-<span class="hljs-built_in">dev</span> libboost-<span class="hljs-built_in">all</span>-<span class="hljs-built_in">dev</span> \<br>libdirectfb-<span class="hljs-built_in">dev</span> libst-<span class="hljs-built_in">dev</span> mesa-utils xvfb x11vnc python3-pip<br><br>python3 -m pip install --upgrade pip setuptools psutil wheel<br></code></pre></div></td></tr></table></figure><h2 id="安装gfootball">安装GFootball</h2><figure class="highlight cmake"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cmake">python3 -m pip <span class="hljs-keyword">install</span> gfootball<br></code></pre></div></td></tr></table></figure><blockquote><p>报错1 ERROR: ERROR: Failed to build installable wheels for somepyproject.toml based projects (gfootball)</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241104191115218.png" /></p><p>尝试解决方案1：更新wheel版本,0.44降到0.38.4，无效</p><p>尝试解决方案2：先将GRF的代码仓clone下来，无效</p><p>尝试解决方案3：无效</p><p>查看cmake版本：<code>cmake --version</code>，报错如下图</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241104194836124.png" /></p><p>但使用dpkg能够查到cmake，<code>dpkg -l | grep -E "cmake"</code>，如下图</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241104195030398.png" /></p><p>使用<code>echo $PATH</code>发现cmake没有配置环境变量，修改.bashrc也无效（<code>export PATH="/usr/bin:$PATH"</code>）</p><p>尝试解决方案4：删除cmake，重新安装——有效</p><p>删除cmake，<code>sudo apt remove cmake</code></p><p>重新安装cmake，<code>sudo apt-get install cmake</code></p></blockquote><blockquote><p>报错2 ModuleNotFoundError: No module named 'skbuild'</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241104203225207.png" /></p><p>尝试解决方案1：安装<code>skbuild</code>，<code>pip install scikit-build</code>，似乎有效。。。（反正用新终端再执行命令时，出现新错误了）</p></blockquote><blockquote><p>报错3 Cannot find boost python</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241104204540414.png" /></p><p>参考issue：https://github.com/google-research/football/issues/310、https://github.com/google-research/football/issues/194</p><p>尝试解决方案1：ubuntu18.04，创建一个python=3.6的虚拟环境。无效</p><p>尝试解决方案2：clone代码仓，使用<code>python3 -m pip install .</code>安装。无效</p><p>尝试解决方案3：修改cmakelist，使用<code>find . -name CMakeLists.txt</code>找到cmakelist，使用<code>find_package(PythonLibs 3.6 REQUIRED)</code>替换<code>find_package(PythonLibs 3 REQUIRED)</code>。无效</p><p>尝试解决方案4：安装3.12版本cmake。参考https://blog.csdn.net/weixin_42035347/article/details/125763949</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">下载CMake3.12（不需要解压到特定路径）<br>wget https://github.com/Kitware/CMake/releases/download/v3.12.0/cmake-3.12.0.tar.gz<br>或者 wget https://cmake.org/files/v3.12/cmake-3.12.0-Linux-x86_64.tar.gz<br>tar -zxvf cmake-3.12.0.tar.gz<br><span class="hljs-built_in">cd</span> cmake-3.12.0<br><br>编译和安装CMake<br>./bootstrap<br>make -j$(<span class="hljs-built_in">nproc</span>)<br><span class="hljs-built_in">sudo</span> make install<br></code></pre></div></td></tr></table></figure><p>尝试解决方案5：参考https://blog.csdn.net/qq_26565435/article/details/129040032</p><p>在<code>find_package</code>语句前添加<code>set(Boost_DEBUG ON)</code>，发现代码找Boost时，找到了anaconda/lib/cmake路径下（原因不明），修改文件名，让编译时找不到anaconda下的cmake</p><p>尝试解决方案6：参考issuehttps://github.com/google-research/football/issues/317，完美解决（python=3.6，跟ubuntu18.04匹配）</p><p><code>conda install py-boost</code></p></blockquote><h2 id="运行示例程序">运行示例程序</h2><p><code>python3 -m gfootball.play_game --action_set=full</code></p><blockquote><p>报错4 signal 11</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241106145842569.png" /></p><p>尝试解决方案1：参考issue：https://github.com/google-research/football/issues/154，无效</p><p>尝试解决方案2：参考issue：https://github.com/google-research/football/issues/205，有效</p><p>添加参数<code>--render=False</code></p><p>尝试解决方案3：参考https://blog.csdn.net/FRIGIDWINTER/article/details/129301640，终端无反应，看起来无效</p></blockquote><h2 id="总结">总结</h2><ol type="1"><li>conda环境python版本和ubuntu默认python版本一致，例如ubuntu=18.04，使用python=3.6</li><li>conda环境中手动<code>conda install py-boost</code></li><li>在服务器端运行环境时关闭渲染<code>--render=False</code></li></ol>]]></content>
    
    
    <categories>
      
      <category>MARL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MARL</tag>
      
      <tag>env</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pymarl源码解读</title>
    <link href="/2024/11/11/pymarl/"/>
    <url>/2024/11/11/pymarl/</url>
    
    <content type="html"><![CDATA[<h1 id="pymarl源码解读">pymarl源码解读</h1><h2 id="源代码仓">源代码仓</h2><blockquote><p>https://github.com/oxwhirl/pymarl</p></blockquote><p>实现算法：</p><ul><li><a href="https://arxiv.org/abs/1803.11485"><strong>QMIX</strong>:QMIX: Monotonic Value Function Factorisation for Deep Multi-AgentReinforcement Learning</a></li><li><a href="https://arxiv.org/abs/1705.08926"><strong>COMA</strong>:Counterfactual Multi-Agent Policy Gradients</a></li><li><a href="https://arxiv.org/abs/1706.05296"><strong>VDN</strong>:Value-Decomposition Networks For Cooperative Multi-AgentLearning</a></li><li><a href="https://arxiv.org/abs/1511.08779"><strong>IQL</strong>:Independent Q-Learning</a></li><li><a href="https://arxiv.org/abs/1905.05408"><strong>QTRAN</strong>:QTRAN: Learning to Factorize with Transformation for CooperativeMulti-Agent Reinforcement Learning</a></li></ul><h2 id="文件结构">文件结构</h2><blockquote><ol type="1"><li>仅考虑src文件夹下的内容</li><li>关注qmix算法涉及文件</li></ol></blockquote><figure class="highlight stylus"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs stylus"><span class="hljs-attribute">src</span><br>├── components <br>│   ├── __init__<span class="hljs-selector-class">.py</span><br>│   ├── action_selectors<span class="hljs-selector-class">.py</span><span class="hljs-comment">// 选择action</span><br>│   ├── episode_buffer<span class="hljs-selector-class">.py</span><span class="hljs-comment">// &quot;采样样本&quot;数据结构</span><br>│   ├── epsilon_schedules<span class="hljs-selector-class">.py</span><span class="hljs-comment">// epsilon衰减</span><br>│   └── transforms<span class="hljs-selector-class">.py</span><br>├── config<span class="hljs-comment">// 实验配置参数</span><br>│   ├── algs<span class="hljs-comment">// 算法配置参数</span><br>│   │   ├── coma<span class="hljs-selector-class">.yaml</span><br>│   │   ├── iql_beta<span class="hljs-selector-class">.yaml</span><br>│   │   ├── iql<span class="hljs-selector-class">.yaml</span><br>│   │   ├── qmix_beta<span class="hljs-selector-class">.yaml</span><br>│   │   ├── qmix<span class="hljs-selector-class">.yaml</span><br>│   │   ├── qtran<span class="hljs-selector-class">.yaml</span><br>│   │   ├── vdn_beta<span class="hljs-selector-class">.yaml</span><br>│   │   └── vdn<span class="hljs-selector-class">.yaml</span><br>│   ├── envs<span class="hljs-comment">// 环境配置参数</span><br>│   │   ├── sc2_beta<span class="hljs-selector-class">.yaml</span><br>│   │   └── sc2<span class="hljs-selector-class">.yaml</span><br>│   └── default<span class="hljs-selector-class">.yaml</span><span class="hljs-comment">// 基础配置参数</span><br>├── controllers<br>│   ├── __init__<span class="hljs-selector-class">.py</span><br>│   └── basic_controller<span class="hljs-selector-class">.py</span><span class="hljs-comment">// agent控制器，从构建agent到选择action</span><br>├── envs<br>│   ├── __init__<span class="hljs-selector-class">.py</span><br>│   └── multiagentenv<span class="hljs-selector-class">.py</span><br>├── learners<span class="hljs-comment">// 训练模型</span><br>│   ├── __init__<span class="hljs-selector-class">.py</span><br>│   ├── coma_learner<span class="hljs-selector-class">.py</span><br>│   ├── q_learner<span class="hljs-selector-class">.py</span><span class="hljs-comment">// 基于q函数训练，包括vdn和qmix</span><br>│   └── qtran_learner<span class="hljs-selector-class">.py</span><br>├── modules<br>│   ├── agents<br>│   │   ├── __init__<span class="hljs-selector-class">.py</span><br>│   │   └── rnn_agent<span class="hljs-selector-class">.py</span><span class="hljs-comment">// agent网络模型，输入观测等信息，输出q值</span><br>│   ├── critics<br>│   │   ├── __init__<span class="hljs-selector-class">.py</span><br>│   │   └── coma<span class="hljs-selector-class">.py</span><br>│   ├── mixers<span class="hljs-comment">// mixing网络</span><br>│   │   ├── __init__<span class="hljs-selector-class">.py</span><br>│   │   ├── qmix<span class="hljs-selector-class">.py</span><span class="hljs-comment">// qmix的mixing网络，论文Figure 2a</span><br>│   │   ├── qtran<span class="hljs-selector-class">.py</span><br>│   │   └── vdn<span class="hljs-selector-class">.py</span><br>│   └── __init__<span class="hljs-selector-class">.py</span><br>├── runners<span class="hljs-comment">// 游戏环境运行</span><br>│   ├── __init__<span class="hljs-selector-class">.py</span><br>│   ├── episode_runner<span class="hljs-selector-class">.py</span><span class="hljs-comment">// 单幕运行，run函数完整运行一次游戏</span><br>│   └── parallel_runner<span class="hljs-selector-class">.py</span><span class="hljs-comment">// 多幕并行</span><br>└── utils<span class="hljs-comment">// 工具函数</span><br>│   ├── dict2namedtuple<span class="hljs-selector-class">.py</span><br>│   ├── logging<span class="hljs-selector-class">.py</span><br>│   ├── rl_utils<span class="hljs-selector-class">.py</span><br>│   └── timehelper<span class="hljs-selector-class">.py</span><br>├── __init__<span class="hljs-selector-class">.py</span><br>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span><span class="hljs-comment">// 程序入口，设置sacred实验</span><br>└── run<span class="hljs-selector-class">.py</span><span class="hljs-comment">// 实验运行，涉及启动实验到关闭环境全流程</span><br></code></pre></div></td></tr></table></figure><h2 id="主要模块介绍">主要模块介绍</h2><blockquote><ol type="1"><li>以qmix算法为例</li><li>不关注log模块</li><li>一些简单的函数，或者工具函数（跟算法思想没有太大关联，可直接复用的代码），也不关注</li></ol></blockquote><h3 id="yaml配置文件">yaml配置文件</h3><ol type="1"><li><p>default.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">use_tensorboard:</span> <span class="hljs-literal">True</span> <span class="hljs-comment"># 使用tensorboard记录实验数据，方便后续分析。</span><br><span class="hljs-attr">save_model:</span> <span class="hljs-literal">True</span> <span class="hljs-comment"># 保存模型，方便后续测试</span><br></code></pre></div></td></tr></table></figure></li></ol><h3 id="程序入口main.py">程序入口——main.py</h3><ol type="1"><li><p>创建sacred实验</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">SETTINGS[<span class="hljs-string">&#x27;CAPTURE_MODE&#x27;</span>] = <span class="hljs-string">&quot;fd&quot;</span> <span class="hljs-comment"># set to &quot;no&quot; if you want to see stdout/stderr in console</span><br>logger = get_logger()   <span class="hljs-comment"># 该语句会导致控制台输出一些类似 &quot;[DEBUG xx:xx:xx] git.cmd Popen(...)&quot; 格式的日志</span><br><br>ex = Experiment(<span class="hljs-string">&quot;pymarl&quot;</span>)<br>ex.logger = logger<br>ex.captured_out_filter = apply_backspaces_and_linefeeds <span class="hljs-comment"># 设置输出格式，避免有些实时输出（进度条等）不适合文件输出的形式</span><br><br>results_path = os.path.join(dirname(dirname(abspath(__file__))), <span class="hljs-string">&quot;results&quot;</span>)<br></code></pre></div></td></tr></table></figure></li><li><p>加载实验配置参数，运行sacred实验</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    params = deepcopy(sys.argv) <span class="hljs-comment"># 接收命令行参数</span><br><br>    <span class="hljs-comment"># Get the defaults from default.yaml</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(os.path.dirname(__file__), <span class="hljs-string">&quot;config&quot;</span>, <span class="hljs-string">&quot;default.yaml&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">try</span>:<br>            config_dict = yaml.load(f, Loader=yaml.FullLoader)<br>        <span class="hljs-keyword">except</span> yaml.YAMLError <span class="hljs-keyword">as</span> exc:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-literal">False</span>, <span class="hljs-string">&quot;default.yaml error: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(exc)<br><br>    <span class="hljs-comment"># Load algorithm and env base configs</span><br>    env_config = _get_config(params, <span class="hljs-string">&quot;--env-config&quot;</span>, <span class="hljs-string">&quot;envs&quot;</span>)    <span class="hljs-comment"># 获取实验环境(e.g.SC2)yaml配置</span><br>    alg_config = _get_config(params, <span class="hljs-string">&quot;--config&quot;</span>, <span class="hljs-string">&quot;algs&quot;</span>)    <span class="hljs-comment"># 获取实验算法(e.g.QMIX)yaml配置</span><br>    config_dict = &#123;**config_dict, **env_config, **alg_config&#125;   <span class="hljs-comment"># 字典合并</span><br>    config_dict = recursive_dict_update(config_dict, env_config)<br>    config_dict = recursive_dict_update(config_dict, alg_config)<br><br>    <span class="hljs-comment"># now add all the config to sacred</span><br>    ex.add_config(config_dict)<br><br>    <span class="hljs-comment"># Save to disk by default for sacred</span><br>    logger.info(<span class="hljs-string">&quot;Saving to FileStorageObserver in results/sacred.&quot;</span>)<br>    file_obs_path = os.path.join(results_path, <span class="hljs-string">&quot;sacred&quot;</span>)<br>    ex.observers.append(FileStorageObserver.create(file_obs_path))  <span class="hljs-comment"># 创建一个ex的观察者文件（写日志）</span><br><br>    ex.run_commandline(params)<br></code></pre></div></td></tr></table></figure></li><li><p>初始化随机种子，启动实验框架</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># sacred实验的主函数</span><br><span class="hljs-meta">@ex.main</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_main</span>(<span class="hljs-params">_run, _config, _log</span>):<br>    <span class="hljs-comment"># Setting the random seed throughout the modules</span><br>    config = config_copy(_config)<br>    np.random.seed(config[<span class="hljs-string">&quot;seed&quot;</span>])<br>    th.manual_seed(config[<span class="hljs-string">&quot;seed&quot;</span>])<br>    config[<span class="hljs-string">&#x27;env_args&#x27;</span>][<span class="hljs-string">&#x27;seed&#x27;</span>] = config[<span class="hljs-string">&quot;seed&quot;</span>]<br><br>    <span class="hljs-comment"># run the framework</span><br>    run(_run, config, _log)<br></code></pre></div></td></tr></table></figure></li></ol><h3 id="实验运行run.py">实验运行——run.py</h3><p>前几行代码基本都是和log相关，可以暂时忽略，核心是<code>run_sequential</code>函数，后面的代码是实验结束后的一些程序上的后处理，与算法无关。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Run and train</span><br>run_sequential(args=args, logger=logger)<br></code></pre></div></td></tr></table></figure><h4 id="run_sequential函数">run_sequential函数</h4><blockquote><p>实验运行的主要函数，构建如下自定义类的对象：</p><p>runner——环境运行器。负责执行游戏环境。</p><p>buffer——经验回放池。负责存放采样数据。</p><p>mac——智能体控制器。负责构建智能体，根据输入选择行为。</p><p>learner——智能体学习器。负责训练模型参数</p><p>最后进行实验，训练智能体，记录实验结果，定期测试并保存模型</p></blockquote><h5id="构造实验需要的各种自定义类对象">构造实验需要的各种自定义类对象</h5><ol type="1"><li><p>定义环境运行器runner</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Init runner so we can get env info</span><br>runner = r_REGISTRY[args.runner](args=args, logger=logger)<br></code></pre></div></td></tr></table></figure></li><li><p>定义采样数据格式，即存在buffer里的数据大概包含哪些信息</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Default/Base scheme</span><br>scheme = &#123;<br>    <span class="hljs-string">&quot;state&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: env_info[<span class="hljs-string">&quot;state_shape&quot;</span>]&#125;,<br>    <span class="hljs-string">&quot;obs&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: env_info[<span class="hljs-string">&quot;obs_shape&quot;</span>], <span class="hljs-string">&quot;group&quot;</span>: <span class="hljs-string">&quot;agents&quot;</span>&#125;,<br>    <span class="hljs-string">&quot;actions&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: (<span class="hljs-number">1</span>,), <span class="hljs-string">&quot;group&quot;</span>: <span class="hljs-string">&quot;agents&quot;</span>, <span class="hljs-string">&quot;dtype&quot;</span>: th.long&#125;,<br>    <span class="hljs-string">&quot;avail_actions&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: (env_info[<span class="hljs-string">&quot;n_actions&quot;</span>],), <span class="hljs-string">&quot;group&quot;</span>: <span class="hljs-string">&quot;agents&quot;</span>, <span class="hljs-string">&quot;dtype&quot;</span>: th.<span class="hljs-built_in">int</span>&#125;,<br>    <span class="hljs-string">&quot;reward&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: (<span class="hljs-number">1</span>,)&#125;,<br>    <span class="hljs-string">&quot;terminated&quot;</span>: &#123;<span class="hljs-string">&quot;vshape&quot;</span>: (<span class="hljs-number">1</span>,), <span class="hljs-string">&quot;dtype&quot;</span>: th.uint8&#125;,<br>&#125;<br>groups = &#123;<br>    <span class="hljs-string">&quot;agents&quot;</span>: args.n_agents<br>&#125;<br>preprocess = &#123;<br>    <span class="hljs-string">&quot;actions&quot;</span>: (<span class="hljs-string">&quot;actions_onehot&quot;</span>, [OneHot(out_dim=args.n_actions)])<br>&#125;<br></code></pre></div></td></tr></table></figure></li><li><p>定义经验回放池buffer</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">buffer = ReplayBuffer(scheme, groups, args.buffer_size, env_info[<span class="hljs-string">&quot;episode_limit&quot;</span>] + <span class="hljs-number">1</span>,<br>                          preprocess=preprocess,<br>                          device=<span class="hljs-string">&quot;cpu&quot;</span> <span class="hljs-keyword">if</span> args.buffer_cpu_only <span class="hljs-keyword">else</span> args.device)<br></code></pre></div></td></tr></table></figure></li><li><p>定义智能体控制器mac</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Setup multiagent controller here</span><br>mac = mac_REGISTRY[args.mac](buffer.scheme, groups, args)<br></code></pre></div></td></tr></table></figure></li><li><p>将上面定义的scheme等信息，以及mac对象传给runner</p><blockquote><p>self.new_batch是一个固定参数的EpisodeBatch类的构造函数，每次调用创建一个新的对象，用于存储采样数据</p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 以EpisodeRunner类的setup函数为例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup</span>(<span class="hljs-params">self, scheme, groups, preprocess, mac</span>):<br>    <span class="hljs-variable language_">self</span>.new_batch = partial(EpisodeBatch, scheme, groups, <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-variable language_">self</span>.episode_limit + <span class="hljs-number">1</span>,<br>                             preprocess=preprocess, device=<span class="hljs-variable language_">self</span>.args.device)<br>    <span class="hljs-variable language_">self</span>.mac = mac<br></code></pre></div></td></tr></table></figure></li><li><p>定义智能体学习器learner</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Learner</span><br>learner = le_REGISTRY[args.learner](mac, buffer.scheme, logger, args)<br></code></pre></div></td></tr></table></figure></li></ol><h5id="如果有保存模型读取模型继续训练">如果有保存模型，读取模型，继续训练</h5><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.checkpoint_path != <span class="hljs-string">&quot;&quot;</span>:<br><br>    timesteps = []<br>    timestep_to_load = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(args.checkpoint_path):<br>        logger.console_logger.info(<span class="hljs-string">&quot;Checkpoint directiory &#123;&#125; doesn&#x27;t exist&quot;</span>.<span class="hljs-built_in">format</span>(args.checkpoint_path))<br>        <span class="hljs-keyword">return</span><br><br>    <span class="hljs-comment"># Go through all files in args.checkpoint_path</span><br>    <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> os.listdir(args.checkpoint_path):<br>        full_name = os.path.join(args.checkpoint_path, name)<br>        <span class="hljs-comment"># Check if they are dirs the names of which are numbers</span><br>        <span class="hljs-keyword">if</span> os.path.isdir(full_name) <span class="hljs-keyword">and</span> name.isdigit():<br>            timesteps.append(<span class="hljs-built_in">int</span>(name))<span class="hljs-comment"># 记录保存的每个模型对应的环境步</span><br><br>    <span class="hljs-comment"># 确定加载哪个环境步的模型</span><br>    <span class="hljs-comment"># 如果load_step参数设置为0，则加载最大的环境步</span><br>  <span class="hljs-comment"># 否则，加载距离load_step最近的环境步保存的模型</span><br>    <span class="hljs-keyword">if</span> args.load_step == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># choose the max timestep</span><br>        timestep_to_load = <span class="hljs-built_in">max</span>(timesteps)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># choose the timestep closest to load_step</span><br>        timestep_to_load = <span class="hljs-built_in">min</span>(timesteps, key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">abs</span>(x - args.load_step))<br><br>    model_path = os.path.join(args.checkpoint_path, <span class="hljs-built_in">str</span>(timestep_to_load))<br><br>    logger.console_logger.info(<span class="hljs-string">&quot;Loading model from &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(model_path))<br>    learner.load_models(model_path)<br>    runner.t_env = timestep_to_load<span class="hljs-comment"># 从保存模型的环境步继续训练</span><br><br>    <span class="hljs-comment"># 仅测试，不训练</span><br>    <span class="hljs-keyword">if</span> args.evaluate <span class="hljs-keyword">or</span> args.save_replay:<br>        evaluate_sequential(args, runner)<br>        <span class="hljs-keyword">return</span><br></code></pre></div></td></tr></table></figure><h5 id="开始训练">开始训练</h5><ol type="1"><li><p>实验参数</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">episode = <span class="hljs-number">0</span><span class="hljs-comment"># 当前训练多少幕</span><br>last_test_T = -args.test_interval - <span class="hljs-number">1</span><span class="hljs-comment"># 上次测试环境步，用于判断是否要进行测试</span><br>last_log_T = <span class="hljs-number">0</span><span class="hljs-comment"># 上次输出日志环境步，用于判断是否要输出日志</span><br>model_save_time = <span class="hljs-number">0</span><span class="hljs-comment"># 上次保存模型环境步，用于判断是否要保存模型</span><br><br>start_time = time.time()<span class="hljs-comment"># 实验开始时间，用于日志信息</span><br>last_time = start_time  <span class="hljs-comment"># 用于计算剩余时间（控制台输出日志）</span><br></code></pre></div></td></tr></table></figure></li><li><p><strong>while循环体（核心）</strong></p><blockquote><p>循环终止条件即训练环境步超出设定阈值</p></blockquote><ol type="1"><li><p>运行游戏环境并保存数据</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Run for a whole episode at a time</span><br>episode_batch = runner.run(test_mode=<span class="hljs-literal">False</span>) <span class="hljs-comment"># 运行一幕</span><br>buffer.insert_episode_batch(episode_batch)<br></code></pre></div></td></tr></table></figure></li><li><p>训练</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> buffer.can_sample(args.batch_size):<span class="hljs-comment"># buffer中数据量超过batch_size</span><br>    episode_sample = buffer.sample(args.batch_size)<span class="hljs-comment"># buffer中存的样本数足够，才会进行训练</span><br><br>    <span class="hljs-comment"># Truncate batch to only filled timesteps</span><br>    max_ep_t = episode_sample.max_t_filled()<br>    episode_sample = episode_sample[:, :max_ep_t]<span class="hljs-comment"># 使用从buffer中采样的训练样本集的最长时间序列，对所有样本的时间维度做截断</span><br><br>    <span class="hljs-keyword">if</span> episode_sample.device != args.device:<br>        episode_sample.to(args.device)<br><br>    learner.train(episode_sample, runner.t_env, episode)<span class="hljs-comment"># 训练</span><br></code></pre></div></td></tr></table></figure></li><li><p>测试</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Execute test runs once in a while</span><br>n_test_runs = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, args.test_nepisode // runner.batch_size)<span class="hljs-comment"># 每次测试跑n_test_runs幕</span><br><span class="hljs-keyword">if</span> (runner.t_env - last_test_T) / args.test_interval &gt;= <span class="hljs-number">1.0</span>:<span class="hljs-comment"># 距离上次测试，已经过test_interval环境步</span><br><br>    logger.console_logger.info(<span class="hljs-string">&quot;t_env: &#123;&#125; / &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(runner.t_env, args.t_max))<span class="hljs-comment"># 控制台打印训练进度</span><br>    logger.console_logger.info(<span class="hljs-string">&quot;Estimated time left: &#123;&#125;. Time passed: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<br>        time_left(last_time, last_test_T, runner.t_env, args.t_max), time_str(time.time() - start_time)))<span class="hljs-comment"># 控制台打印估计剩余训练时间</span><br>    last_time = time.time()<br><br>    last_test_T = runner.t_env<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_test_runs):<br>        runner.run(test_mode=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 测试</span><br></code></pre></div></td></tr></table></figure></li><li><p>保存模型</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.save_model <span class="hljs-keyword">and</span> (runner.t_env - model_save_time &gt;= args.save_model_interval <span class="hljs-keyword">or</span> model_save_time == <span class="hljs-number">0</span>):<span class="hljs-comment"># 超参数设置save_model并且距离上次保存模型，已经过save_model_interval环境步（或者是训练的起始阶段）</span><br>    model_save_time = runner.t_env<br>    save_path = os.path.join(args.local_results_path, <span class="hljs-string">&quot;models&quot;</span>, args.unique_token, <span class="hljs-built_in">str</span>(runner.t_env))<br>    <span class="hljs-comment">#&quot;results/models/&#123;&#125;&quot;.format(unique_token)</span><br>    os.makedirs(save_path, exist_ok=<span class="hljs-literal">True</span>)<br>    logger.console_logger.info(<span class="hljs-string">&quot;Saving models to &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(save_path))<br><br>    <span class="hljs-comment"># learner should handle saving/loading -- delegate actor save/load to mac,</span><br>    <span class="hljs-comment"># use appropriate filenames to do critics, optimizer states</span><br>    learner.save_models(save_path)<span class="hljs-comment"># 保存模型</span><br></code></pre></div></td></tr></table></figure></li><li><p>打印日志</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> (runner.t_env - last_log_T) &gt;= args.log_interval:<span class="hljs-comment"># 距离上次打印日志，已经过log_interval环境步</span><br>    logger.log_stat(<span class="hljs-string">&quot;episode&quot;</span>, episode, runner.t_env)<br>    logger.print_recent_stats()<br>    last_log_T = runner.t_env<br></code></pre></div></td></tr></table></figure></li></ol></li><li><p>关闭环境</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">runner.close_env()<br>logger.console_logger.info(<span class="hljs-string">&quot;Finished Training&quot;</span>)<br></code></pre></div></td></tr></table></figure></li></ol><h3id="游戏环境运行episode_runner.py">游戏环境运行——episode_runner.py</h3><blockquote><p>pymarl框架总共有两种环境运行器，见<code>src/runners/__init__.py</code>文件</p><p><code>EpisodeRunner</code>一次运行一幕游戏（下面介绍的是这个类）</p><p><code>ParallelRunner</code>一次运行多幕游戏（由<code>default.yaml</code>中的<code>batch_size_run</code>参数控制）</p></blockquote><h4 id="init__函数">__init__函数</h4><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.args = args<br><span class="hljs-variable language_">self</span>.logger = logger<br><span class="hljs-variable language_">self</span>.batch_size = <span class="hljs-variable language_">self</span>.args.batch_size_run<br><span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.batch_size == <span class="hljs-number">1</span><br><br><span class="hljs-variable language_">self</span>.env = env_REGISTRY[<span class="hljs-variable language_">self</span>.args.env](**<span class="hljs-variable language_">self</span>.args.env_args)<br><span class="hljs-variable language_">self</span>.episode_limit = <span class="hljs-variable language_">self</span>.env.episode_limit<br><span class="hljs-variable language_">self</span>.t = <span class="hljs-number">0</span><span class="hljs-comment"># 时间步，记录当前游戏执行了多少步</span><br><br><span class="hljs-variable language_">self</span>.t_env = <span class="hljs-number">0</span><span class="hljs-comment"># 环境步，记录整个实验总共执行了多少时间步</span><br><br><span class="hljs-variable language_">self</span>.train_returns = []<br><span class="hljs-variable language_">self</span>.test_returns = []<br><span class="hljs-variable language_">self</span>.train_stats = &#123;&#125;<br><span class="hljs-variable language_">self</span>.test_stats = &#123;&#125;<br><br><span class="hljs-comment"># Log the first run</span><br><span class="hljs-variable language_">self</span>.log_train_stats_t = -<span class="hljs-number">1000000</span><br></code></pre></div></td></tr></table></figure><h4 id="run函数">run函数</h4><ol type="1"><li><p>初始化</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.reset()<span class="hljs-comment"># 重置环境</span><br><br>terminated = <span class="hljs-literal">False</span><span class="hljs-comment"># 标记游戏是否结束</span><br>episode_return = <span class="hljs-number">0</span><span class="hljs-comment"># 记录当前幕累计return</span><br><span class="hljs-variable language_">self</span>.mac.init_hidden(batch_size=<span class="hljs-variable language_">self</span>.batch_size)<span class="hljs-comment"># 重置agent隐状态，避免将上局游戏&quot;记忆&quot;带入本局游戏</span><br></code></pre></div></td></tr></table></figure></li><li><p>进行一幕游戏，直到游戏结束</p><blockquote><p>获取环境信息并存储——&gt;选择action——&gt;环境递进一步存储信息——&gt;...</p><p><imgsrc="https://cdn.jsdelivr.net/gh/VON-z/image_hosting//SIGS/image-20241217162406103.png" /></p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminated:<br><br>    pre_transition_data = &#123;<br>        <span class="hljs-string">&quot;state&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_state()],<br>        <span class="hljs-string">&quot;avail_actions&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_avail_actions()],<br>        <span class="hljs-string">&quot;obs&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_obs()]<br>    &#125;<br><br>    <span class="hljs-variable language_">self</span>.batch.update(pre_transition_data, ts=<span class="hljs-variable language_">self</span>.t)<br><br>    <span class="hljs-comment"># Pass the entire batch of experiences up till now to the agents</span><br>    <span class="hljs-comment"># Receive the actions for each agent at this timestep in a batch of size 1</span><br>    actions = <span class="hljs-variable language_">self</span>.mac.select_actions(<span class="hljs-variable language_">self</span>.batch, t_ep=<span class="hljs-variable language_">self</span>.t, t_env=<span class="hljs-variable language_">self</span>.t_env, test_mode=test_mode)<br><br>    reward, terminated, env_info = <span class="hljs-variable language_">self</span>.env.step(actions[<span class="hljs-number">0</span>])<span class="hljs-comment"># 根据agents行为，环境递进一步</span><br>    episode_return += reward<span class="hljs-comment"># 累积幕return</span><br><br>    post_transition_data = &#123;<br>        <span class="hljs-string">&quot;actions&quot;</span>: actions,<br>        <span class="hljs-string">&quot;reward&quot;</span>: [(reward,)],<br>        <span class="hljs-string">&quot;terminated&quot;</span>: [(terminated != env_info.get(<span class="hljs-string">&quot;episode_limit&quot;</span>, <span class="hljs-literal">False</span>),)],<br>    &#125;<br><br>    <span class="hljs-variable language_">self</span>.batch.update(post_transition_data, ts=<span class="hljs-variable language_">self</span>.t)<br><br>    <span class="hljs-variable language_">self</span>.t += <span class="hljs-number">1</span><span class="hljs-comment"># 时间步+1</span><br><br>last_data = &#123;<br>    <span class="hljs-string">&quot;state&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_state()],<br>    <span class="hljs-string">&quot;avail_actions&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_avail_actions()],<br>    <span class="hljs-string">&quot;obs&quot;</span>: [<span class="hljs-variable language_">self</span>.env.get_obs()]<br>&#125;<br><span class="hljs-variable language_">self</span>.batch.update(last_data, ts=<span class="hljs-variable language_">self</span>.t)<br><br><span class="hljs-comment"># Select actions in the last stored state</span><br>actions = <span class="hljs-variable language_">self</span>.mac.select_actions(<span class="hljs-variable language_">self</span>.batch, t_ep=<span class="hljs-variable language_">self</span>.t, t_env=<span class="hljs-variable language_">self</span>.t_env, test_mode=test_mode)<br><span class="hljs-variable language_">self</span>.batch.update(&#123;<span class="hljs-string">&quot;actions&quot;</span>: actions&#125;, ts=<span class="hljs-variable language_">self</span>.t)<br></code></pre></div></td></tr></table></figure></li><li><p>日志相关操作（忽略）</p></li></ol><h3 id="采样样本episode_buffer.py">采样样本——episode_buffer.py</h3><h4 id="样本数据结构episodebatch类">样本数据结构——EpisodeBatch类</h4><p>:sweat_smile:</p><h4 id="经验回放池replaybuffer类">经验回放池——ReplayBuffer类</h4><blockquote><p>继承<code>EpisodeBatch</code>类，在此基础上添加了经验回放池的功能</p></blockquote><ol type="1"><li><p>初始化</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, scheme, groups, buffer_size, max_seq_length, preprocess=<span class="hljs-literal">None</span>, device=<span class="hljs-string">&quot;cpu&quot;</span></span>):<br>    <span class="hljs-built_in">super</span>(ReplayBuffer, <span class="hljs-variable language_">self</span>).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)<br>    <span class="hljs-variable language_">self</span>.buffer_size = buffer_size  <span class="hljs-comment"># same as self.batch_size but more explicit</span><br>    <span class="hljs-variable language_">self</span>.buffer_index = <span class="hljs-number">0</span>   <span class="hljs-comment"># 插入数据的起始index</span><br>    <span class="hljs-variable language_">self</span>.episodes_in_buffer = <span class="hljs-number">0</span> <span class="hljs-comment"># 当前buffer中存有多少episode</span><br></code></pre></div></td></tr></table></figure></li><li><p>添加数据</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 以self.buffer_size为模数循环添加数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">insert_episode_batch</span>(<span class="hljs-params">self, ep_batch</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.buffer_index + ep_batch.batch_size &lt;= <span class="hljs-variable language_">self</span>.buffer_size:<br>        <span class="hljs-comment"># 当待添加数据不超过buffer_size时，直接加进去</span><br>        <span class="hljs-variable language_">self</span>.update(ep_batch.data.transition_data,<br>                    <span class="hljs-built_in">slice</span>(<span class="hljs-variable language_">self</span>.buffer_index, <span class="hljs-variable language_">self</span>.buffer_index + ep_batch.batch_size),<br>                    <span class="hljs-built_in">slice</span>(<span class="hljs-number">0</span>, ep_batch.max_seq_length),<br>                    mark_filled=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.update(ep_batch.data.episode_data,<br>                    <span class="hljs-built_in">slice</span>(<span class="hljs-variable language_">self</span>.buffer_index, <span class="hljs-variable language_">self</span>.buffer_index + ep_batch.batch_size))<br>        <span class="hljs-variable language_">self</span>.buffer_index = (<span class="hljs-variable language_">self</span>.buffer_index + ep_batch.batch_size)<span class="hljs-comment"># 更新插入位置索引</span><br>        <span class="hljs-variable language_">self</span>.episodes_in_buffer = <span class="hljs-built_in">max</span>(<span class="hljs-variable language_">self</span>.episodes_in_buffer, <span class="hljs-variable language_">self</span>.buffer_index)   <span class="hljs-comment"># 当buffer满了以后，episode_in_buffer恒为5000，而buffer_index会循环计算</span><br>        <span class="hljs-variable language_">self</span>.buffer_index = <span class="hljs-variable language_">self</span>.buffer_index % <span class="hljs-variable language_">self</span>.buffer_size<span class="hljs-comment"># 当插入位置索引到5000后，归0</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.buffer_index &lt; <span class="hljs-variable language_">self</span>.buffer_size<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 当待添加数据超过buffer_size时，截断</span><br>        <span class="hljs-comment"># 一部分直接加进去，多余部分，从buffer的头部开始添加</span><br>        buffer_left = <span class="hljs-variable language_">self</span>.buffer_size - <span class="hljs-variable language_">self</span>.buffer_index<br>        <span class="hljs-variable language_">self</span>.insert_episode_batch(ep_batch[<span class="hljs-number">0</span>:buffer_left, :])<br>        <span class="hljs-variable language_">self</span>.insert_episode_batch(ep_batch[buffer_left:, :])<br></code></pre></div></td></tr></table></figure></li><li><p>采样数据</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.can_sample(batch_size)<span class="hljs-comment"># 判断经验回放池中存有足量数据</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.episodes_in_buffer == batch_size:<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>[:batch_size]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Uniform sampling only atm</span><br>        ep_ids = np.random.choice(<span class="hljs-variable language_">self</span>.episodes_in_buffer, batch_size, replace=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>[ep_ids]<br></code></pre></div></td></tr></table></figure></li></ol><h3id="智能体控制器basic_controller.py">智能体控制器——basic_controller.py</h3><blockquote><p>This multi-agent controller shares parameters between agents</p></blockquote><h4 id="init__函数-1">__init__函数</h4><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.n_agents = args.n_agents<span class="hljs-comment"># 智能体数量</span><br><span class="hljs-variable language_">self</span>.args = args<span class="hljs-comment"># 实验参数</span><br>input_shape = <span class="hljs-variable language_">self</span>._get_input_shape(scheme)<span class="hljs-comment"># 获取输入维度，3.6.5介绍</span><br><span class="hljs-variable language_">self</span>._build_agents(input_shape)<span class="hljs-comment"># 创建智能体网络（RNN模型，价值网络）</span><br><span class="hljs-variable language_">self</span>.agent_output_type = args.agent_output_type<br><br><span class="hljs-variable language_">self</span>.action_selector = action_REGISTRY[args.action_selector](args)<span class="hljs-comment"># action选择器，e.g. epsilon-greedy</span><br><br><span class="hljs-variable language_">self</span>.hidden_states = <span class="hljs-literal">None</span><span class="hljs-comment"># RNN模型的隐状态</span><br></code></pre></div></td></tr></table></figure><h4 id="select_actions函数">select_actions函数</h4><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Only select actions for the selected batch elements in bs</span><br>avail_actions = ep_batch[<span class="hljs-string">&quot;avail_actions&quot;</span>][:, t_ep]<span class="hljs-comment"># 当前时刻可执行动作</span><br>agent_outputs = <span class="hljs-variable language_">self</span>.forward(ep_batch, t_ep, test_mode=test_mode)<span class="hljs-comment"># agent模型前向传播，3.6.3介绍</span><br>chosen_actions = <span class="hljs-variable language_">self</span>.action_selector.select_action(agent_outputs[bs], avail_actions[bs], t_env, test_mode=test_mode)<span class="hljs-comment"># 选择action</span><br><span class="hljs-keyword">return</span> chosen_actions<br></code></pre></div></td></tr></table></figure><h4 id="forward函数">forward函数</h4><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">agent_inputs = <span class="hljs-variable language_">self</span>._build_inputs(ep_batch, t)<span class="hljs-comment"># 构建智能体t时刻观测，3.6.4介绍</span><br>avail_actions = ep_batch[<span class="hljs-string">&quot;avail_actions&quot;</span>][:, t]<span class="hljs-comment"># 当前时刻可执行动作</span><br>agent_outs, <span class="hljs-variable language_">self</span>.hidden_states = <span class="hljs-variable language_">self</span>.agent(agent_inputs, <span class="hljs-variable language_">self</span>.hidden_states)<br><br><span class="hljs-comment"># 中间代码针对coma算法，省略</span><br><br><span class="hljs-keyword">return</span> agent_outs.view(ep_batch.batch_size, <span class="hljs-variable language_">self</span>.n_agents, -<span class="hljs-number">1</span>)<br></code></pre></div></td></tr></table></figure><h4 id="build_inputs函数">_build_inputs函数</h4><blockquote><p>构建智能体t时刻的观测</p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Assumes homogenous agents with flat observations.</span><br><span class="hljs-comment"># Other MACs might want to e.g. delegate building inputs to each agent</span><br>bs = batch.batch_size<br>inputs = []<br>inputs.append(batch[<span class="hljs-string">&quot;obs&quot;</span>][:, t])  <span class="hljs-comment"># b1av，分别对应batch_size, time_step, agent, vshape</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.obs_last_action:<span class="hljs-comment"># 观测包含agent的上一个动作</span><br>    <span class="hljs-keyword">if</span> t == <span class="hljs-number">0</span>:<br>        inputs.append(th.zeros_like(batch[<span class="hljs-string">&quot;actions_onehot&quot;</span>][:, t]))<br>    <span class="hljs-keyword">else</span>:<br>        inputs.append(batch[<span class="hljs-string">&quot;actions_onehot&quot;</span>][:, t-<span class="hljs-number">1</span>])<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.obs_agent_id:<span class="hljs-comment"># 观测包含agent的id</span><br>    inputs.append(th.eye(<span class="hljs-variable language_">self</span>.n_agents, device=batch.device).unsqueeze(<span class="hljs-number">0</span>).expand(bs, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br><br>inputs = th.cat([x.reshape(bs*<span class="hljs-variable language_">self</span>.n_agents, -<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs], dim=<span class="hljs-number">1</span>)<br><span class="hljs-keyword">return</span> inputs<br></code></pre></div></td></tr></table></figure><h4 id="get_input_shape函数">_get_input_shape函数</h4><blockquote><p>获取agent模型输入的维度</p></blockquote><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">input_shape = scheme[<span class="hljs-string">&quot;obs&quot;</span>][<span class="hljs-string">&quot;vshape&quot;</span>]<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.obs_last_action:<span class="hljs-comment"># 观测包含agent的上一个动作</span><br>    input_shape += scheme[<span class="hljs-string">&quot;actions_onehot&quot;</span>][<span class="hljs-string">&quot;vshape&quot;</span>][<span class="hljs-number">0</span>]<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.obs_agent_id:<span class="hljs-comment"># 观测包含agent的id</span><br>    input_shape += <span class="hljs-variable language_">self</span>.n_agents<br><br><span class="hljs-keyword">return</span> input_shape<br></code></pre></div></td></tr></table></figure><h3 id="训练模型q_learner.py">训练模型——q_learner.py</h3><h4 id="init__函数-2">__init__函数</h4><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.args = args<span class="hljs-comment"># 实验参数</span><br><span class="hljs-variable language_">self</span>.mac = mac<span class="hljs-comment"># 智能体控制器</span><br><span class="hljs-variable language_">self</span>.logger = logger<span class="hljs-comment"># 日志</span><br><br><span class="hljs-variable language_">self</span>.params = <span class="hljs-built_in">list</span>(mac.parameters())<br><br><span class="hljs-variable language_">self</span>.last_target_update_episode = <span class="hljs-number">0</span><br><br><span class="hljs-variable language_">self</span>.mixer = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> args.mixer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<span class="hljs-comment"># Mixing网络</span><br>    <span class="hljs-keyword">if</span> args.mixer == <span class="hljs-string">&quot;vdn&quot;</span>:<br>        <span class="hljs-variable language_">self</span>.mixer = VDNMixer()<br>    <span class="hljs-keyword">elif</span> args.mixer == <span class="hljs-string">&quot;qmix&quot;</span>:<br>        <span class="hljs-variable language_">self</span>.mixer = QMixer(args)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Mixer &#123;&#125; not recognised.&quot;</span>.<span class="hljs-built_in">format</span>(args.mixer))<br>    <span class="hljs-variable language_">self</span>.params += <span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.mixer.parameters())<br>    <span class="hljs-variable language_">self</span>.target_mixer = copy.deepcopy(<span class="hljs-variable language_">self</span>.mixer)<br><br><span class="hljs-variable language_">self</span>.optimiser = RMSprop(params=<span class="hljs-variable language_">self</span>.params, lr=args.lr, alpha=args.optim_alpha, eps=args.optim_eps)<span class="hljs-comment"># 优化器</span><br><br><span class="hljs-comment"># a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC</span><br><span class="hljs-variable language_">self</span>.target_mac = copy.deepcopy(mac)<br><br><span class="hljs-variable language_">self</span>.log_stats_t = -<span class="hljs-variable language_">self</span>.args.learner_log_interval - <span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure><h4 id="train函数重点">train函数（重点）</h4><blockquote><p>部分细节跳过</p></blockquote><ol type="1"><li><p>整理数据</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Get the relevant quantities</span><br>rewards = batch[<span class="hljs-string">&quot;reward&quot;</span>][:, :-<span class="hljs-number">1</span>]<br>actions = batch[<span class="hljs-string">&quot;actions&quot;</span>][:, :-<span class="hljs-number">1</span>]<br>terminated = batch[<span class="hljs-string">&quot;terminated&quot;</span>][:, :-<span class="hljs-number">1</span>].<span class="hljs-built_in">float</span>()<br>mask = batch[<span class="hljs-string">&quot;filled&quot;</span>][:, :-<span class="hljs-number">1</span>].<span class="hljs-built_in">float</span>()<br>mask[:, <span class="hljs-number">1</span>:] = mask[:, <span class="hljs-number">1</span>:] * (<span class="hljs-number">1</span> - terminated[:, :-<span class="hljs-number">1</span>])<br>avail_actions = batch[<span class="hljs-string">&quot;avail_actions&quot;</span>]<br></code></pre></div></td></tr></table></figure></li><li><p>计算训练样本轨迹对应的Q值</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Calculate estimated Q-Values</span><br>mac_out = []<br><span class="hljs-variable language_">self</span>.mac.init_hidden(batch.batch_size)  <span class="hljs-comment"># 训练前，清空mac记录的隐状态</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch.max_seq_length):<br>    agent_outs = <span class="hljs-variable language_">self</span>.mac.forward(batch, t=t)<br>    mac_out.append(agent_outs)<br>mac_out = th.stack(mac_out, dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># Concat over time</span><br></code></pre></div></td></tr></table></figure></li><li><p>提取agent实际选择动作的Q值</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Pick the Q-Values for the actions taken by each agent</span><br>chosen_action_qvals = th.gather(mac_out[:, :-<span class="hljs-number">1</span>], dim=<span class="hljs-number">3</span>, index=actions).squeeze(<span class="hljs-number">3</span>)  <span class="hljs-comment"># Remove the last dim</span><br></code></pre></div></td></tr></table></figure></li><li><p>使用target网络计算训练样本轨迹对应的Q值（训练目标）</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Calculate the Q-Values necessary for the target</span><br>target_mac_out = []<br><span class="hljs-variable language_">self</span>.target_mac.init_hidden(batch.batch_size)<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch.max_seq_length):<br>    target_agent_outs = <span class="hljs-variable language_">self</span>.target_mac.forward(batch, t=t)<br>    target_mac_out.append(target_agent_outs)<br></code></pre></div></td></tr></table></figure></li><li><p>计算TD-error</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Max over target Q-Values</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.double_q:<br>    <span class="hljs-comment"># Get actions that maximise live Q (for double q-learning)</span><br>    mac_out_detach = mac_out.clone().detach()<br>    mac_out_detach[avail_actions == <span class="hljs-number">0</span>] = -<span class="hljs-number">9999999</span><br>    cur_max_actions = mac_out_detach[:, <span class="hljs-number">1</span>:].<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">3</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]<br>    target_max_qvals = th.gather(target_mac_out, <span class="hljs-number">3</span>, cur_max_actions).squeeze(<span class="hljs-number">3</span>)<br><span class="hljs-keyword">else</span>:<br>    target_max_qvals = target_mac_out.<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">3</span>)[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># Mix</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.mixer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    chosen_action_qvals = <span class="hljs-variable language_">self</span>.mixer(chosen_action_qvals, batch[<span class="hljs-string">&quot;state&quot;</span>][:, :-<span class="hljs-number">1</span>])<br>    target_max_qvals = <span class="hljs-variable language_">self</span>.target_mixer(target_max_qvals, batch[<span class="hljs-string">&quot;state&quot;</span>][:, <span class="hljs-number">1</span>:])<br><br><span class="hljs-comment"># Calculate 1-step Q-Learning targets</span><br>targets = rewards + <span class="hljs-variable language_">self</span>.args.gamma * (<span class="hljs-number">1</span> - terminated) * target_max_qvals<br><br><span class="hljs-comment"># Td-error</span><br>td_error = (chosen_action_qvals - targets.detach())<br><br>mask = mask.expand_as(td_error)<br><br><span class="hljs-comment"># 0-out the targets that came from padded data</span><br>masked_td_error = td_error * mask<br></code></pre></div></td></tr></table></figure></li><li><p>优化参数</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Normal L2 loss, take mean over actual data</span><br>loss = (masked_td_error ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / mask.<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-comment"># Optimise</span><br><span class="hljs-variable language_">self</span>.optimiser.zero_grad()<br>loss.backward()<br>grad_norm = th.nn.utils.clip_grad_norm_(<span class="hljs-variable language_">self</span>.params, <span class="hljs-variable language_">self</span>.args.grad_norm_clip)<br><span class="hljs-variable language_">self</span>.optimiser.step()<br></code></pre></div></td></tr></table></figure></li><li><p>更新target网络</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> (episode_num - <span class="hljs-variable language_">self</span>.last_target_update_episode) / <span class="hljs-variable language_">self</span>.args.target_update_interval &gt;= <span class="hljs-number">1.0</span>:<br>    <span class="hljs-variable language_">self</span>._update_targets()<br>    <span class="hljs-variable language_">self</span>.last_target_update_episode = episode_num<br></code></pre></div></td></tr></table></figure></li></ol><h2 id="在本地测试效果">在本地测试效果</h2><blockquote><p><strong>Note:</strong> Replays cannot be watched using the Linuxversion of StarCraft II. Please use either the Mac or Windows version ofthe StarCraft II client.</p><p>根据官方介绍，需要在windows下查看效果</p><p><em>下面是将训练model回传本地做测试，也可以在服务器端进行测试，直接将生成的<code>.SC2Replay</code>文件回传本地，回放文件在<code>/StarCraft II/Replays/</code>路径下面</em></p></blockquote><ol type="1"><li><p>使用<code>scp</code>将训练好的model回传本地</p><figure class="highlight ruby"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs ruby">scp -r &lt;user<span class="hljs-variable">@remote_host</span><span class="hljs-symbol">:/path/to/remote/folder&gt;</span> &lt;<span class="hljs-regexp">/path/to</span><span class="hljs-regexp">/local/destination</span>&gt;<br></code></pre></div></td></tr></table></figure></li><li><p>设置<code>default.yaml</code>参数</p><figure class="highlight vbnet"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs vbnet"><span class="hljs-symbol">checkpoint_path:</span> <span class="hljs-string">&quot;results/models/qmix__2024-11-10_13-17-31&quot;</span><br><span class="hljs-symbol">evaluate:</span> <span class="hljs-literal">True</span><br><span class="hljs-symbol">save_replay:</span> <span class="hljs-literal">True</span><br></code></pre></div></td></tr></table></figure></li><li><p>运行实验（仅测试，生成回放文件）</p><figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros"><span class="hljs-comment"># 参数跟训练时保持一致</span><br>python src/main.py <span class="hljs-attribute">--config</span>=qmix <span class="hljs-attribute">--env-config</span>=sc2 with env_args.<span class="hljs-attribute">map_name</span>=2s3z<br></code></pre></div></td></tr></table></figure></li><li><p>运行<code>.SC2Replay</code>程序</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>MARL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MARL</tag>
      
      <tag>env</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
